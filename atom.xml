<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Ant-loiter个人博客</title>
  
  <subtitle>www.ant-loiter.com</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://www.ant-loiter.com/"/>
  <updated>2020-10-30T13:49:33.119Z</updated>
  <id>https://www.ant-loiter.com/</id>
  
  <author>
    <name>chenhj(brenda)</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Hello World</title>
    <link href="https://www.ant-loiter.com/2020/08/22/hello-world/"/>
    <id>https://www.ant-loiter.com/2020/08/22/hello-world/</id>
    <published>2020-08-22T03:33:11.730Z</published>
    <updated>2020-10-30T13:49:33.119Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p><h3 id="config-page"><a href="#config-page" class="headerlink" title="config page."></a>config page.</h3>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>CAP_principle</title>
    <link href="https://www.ant-loiter.com/2020/07/01/CAP-principle/"/>
    <id>https://www.ant-loiter.com/2020/07/01/CAP-principle/</id>
    <published>2020-07-01T04:06:11.000Z</published>
    <updated>2020-10-30T13:49:33.117Z</updated>
    
    <content type="html"><![CDATA[<h1 id="CAP-原则"><a href="#CAP-原则" class="headerlink" title="CAP 原则"></a>CAP 原则</h1><p>CAP原则又称CAP定理， 指的是在一个分布式系统中， 一致性（Consistency), 可用性（Availability), 分区容错性（Partition Tolerance). CAP原则指的是， 这三个要素最多只能同时实现两点， 不可能三者兼顾。 </p><p>一致性（C， Consistency) 在分布式系统中的所有数据备份， 在同一时刻是否同样的值，即各个节点上的数据都是一样的。 </p><p>可用性（A， Avalibility) 在集群是一部他节点故障后， 集群整体是否还能响应客户端的读写请求， （对于数据更新具备高可用性）</p><p>分区容忍性（P Partition Tolerance), 以实际效果而言， 分析相当于对通信的时限要求， 系统如果不能在时限内达到数据一致性， 就意味着发生了分区的情况， 必须要当前操作在C和A这间作出选择。 </p><p>CAP原则的精髓就是要么AP， 要么CP， 但是不存在CAP。 如果在某个分布系统中数据无副本， 那么系统必然满足强一致性条件， 因为只有独一数据， 不会出现数据不一致的情况， 些时C和P两要素具备， 但是如果系统发生了网络分区状态或者宕机，必须导致某些数据不可以访问， 此时可用性条件就不能被满足， 即在此情况下获到了CP系统，但是CAP不可同时满足。 </p><h2 id="服务注册中心是CP还是AP？"><a href="#服务注册中心是CP还是AP？" class="headerlink" title="服务注册中心是CP还是AP？"></a>服务注册中心是CP还是AP？</h2><p>服务注册中心是为了服务间调用服务的， 那么绝对不允许因为服务注册中心出现了问题而导致服务间的调用出现问题。 </p><p>如果node1, node2, node3集群节点。保存着可用的服务列表ip1,ip2,ip3,试想如果些时不一致， node1只保存了ip1,ip2, 此时服务读取到了node1的节点， 那么会造成什么影响？</p><p>调用node1的服务， 顶多就是负载均衡时不会有流量打到ip3,然后等node1同步回ip3后，又一致了，  这对服务其实没有什么太的影响。 </p><p>所以， 推出服务注册中心应该是一个AP系统。 </p><p>Zookeeper是个CP系统， 强一致性</p><p>场景1：  当master挂了， 此时， zookeeper集群是需要停止对外服务， 启动内部选举， 所以影响到了服务的可用性。 </p><p>当然就可以说服务本地有缓存可用列表，但是下面这种方式就更无法处理了。 </p><p>场景2： 分区可用， 试想， 有3个机房， 如果其中机房3和机房1，2网络断了， 那么机房3的注册中心就不能注册新的机器了么， 这显然不合理。 </p><p><strong>zookeep是通过TCP的心跳来判断服务是否可用，但是TCP的活性并不代表服务是否可用，但TCP活性可以说明服务是可用的么，答案显然是否定的，如：连接池已经满了，DB挂了等情况</strong></p><h2 id="理想的注册中心需要"><a href="#理想的注册中心需要" class="headerlink" title="理想的注册中心需要"></a>理想的注册中心需要</h2><ul><li>服务的自动注册与发现。最好有新的服务注册上去时还能推送到调用端。 </li><li>能对注册上来的机器方便的进行管理，能手工剔除（发送信号让服务优雅下线）、恢复机器</li><li>服务的健康检查， 能真正的检测到服务是否可用</li><li>可以看到是否有其他调用服务正在订阅注册上来的服务</li><li></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;CAP-原则&quot;&gt;&lt;a href=&quot;#CAP-原则&quot; class=&quot;headerlink&quot; title=&quot;CAP 原则&quot;&gt;&lt;/a&gt;CAP 原则&lt;/h1&gt;&lt;p&gt;CAP原则又称CAP定理， 指的是在一个分布式系统中， 一致性（Consistency), 可用性（Avai
      
    
    </summary>
    
      <category term="CAP" scheme="https://www.ant-loiter.com/categories/CAP/"/>
    
    
      <category term="CAP" scheme="https://www.ant-loiter.com/tags/CAP/"/>
    
  </entry>
  
  <entry>
    <title>Netty_notepad_001</title>
    <link href="https://www.ant-loiter.com/2020/06/27/Netty-notepad-001/"/>
    <id>https://www.ant-loiter.com/2020/06/27/Netty-notepad-001/</id>
    <published>2020-06-27T10:03:34.000Z</published>
    <updated>2020-10-30T13:49:33.119Z</updated>
    
    <content type="html"><![CDATA[<h1 id="作者"><a href="#作者" class="headerlink" title="作者"></a>作者</h1><p>  Netty 由 Trustin Lee (韩国， Line公司) 2004年开发</p><h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><ul><li>本质： 网络应用程序框架</li><li>实现： 异步、事件驱动</li><li>特征： 高性能、可维护、快速开发</li><li>用途： 开发服务器和客户端</li></ul><h1 id="Netty的现状与趋势"><a href="#Netty的现状与趋势" class="headerlink" title="Netty的现状与趋势"></a>Netty的现状与趋势</h1><p>一些典型项目：</p><ul><li>大数据： Cassandra</li><li>大数据处理: Spark hadoop</li><li><p>Message Queue: RocketMQ</p></li><li><p>检索： Elasticsearch</p></li><li>框架： gRPC Apache Dubbo Spring5</li><li>分布式协调器： Zookeeper</li><li>工具类： async-http-client</li><li><p>其他参考： <a href="https://netty.io/wiki/adopters.html" target="_blank" rel="noopener">https://netty.io/wiki/adopters.html</a></p></li><li><p>趋势</p></li><li>更多流行协议的支持<ul><li>dns</li><li>haproxy</li><li>http</li><li>http2</li><li>memcache</li><li>mqtt</li><li>redis</li><li>smtp</li><li>socks</li><li>stomp</li><li>xml</li></ul></li><li>紧跟JDK新功能能步代</li><li>更多易用、人性化的功能</li><li>IP 地址黑白名单、流量整形等</li><li>应用越来越多<h1 id="什么时经典的三种I-O模式"><a href="#什么时经典的三种I-O模式" class="headerlink" title="什么时经典的三种I/O模式"></a>什么时经典的三种I/O模式</h1>| | | | |<br>|:—:|:—-:|:—-:|:—-:|<br>|排队打饭模式|BIO（阻塞I/O）| JDK1.4之前|阻塞同步|<br>|点单、等待被叫模式|NIO（非阻塞I/O）| JDK1.4（2002年，java.nio包）| 非阻塞同步|<br>|包厢模式| AIO（异步I/O）| JDK1.7（2011年）| 非阻塞异步|</li></ul><h1 id="Netty-如何支持三种Reactor"><a href="#Netty-如何支持三种Reactor" class="headerlink" title="Netty 如何支持三种Reactor"></a>Netty 如何支持三种Reactor</h1><ul><li>什么是Reactor及三种版本 ， BIO Thread-Per-Connection </li><li>如何在Netty中使用Reactor模式  NIO Reactor </li><li>解析Netty对 Reactor模式的支持的常见疑问  AIO  Proactor </li></ul><h1 id="如何在Netty-中使用Reactor-模式"><a href="#如何在Netty-中使用Reactor-模式" class="headerlink" title="如何在Netty 中使用Reactor 模式"></a>如何在Netty 中使用Reactor 模式</h1><ul><li><p>Reactor 单线程模式</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">EventLoopGroup eventGroup = <span class="keyword">new</span> NioEventLoopGroup(<span class="number">1</span>);</span><br><span class="line">ServerBootstrap serverBootstrap = <span class="keyword">new</span> ServerBootstrap();</span><br><span class="line">serverBootstrap.group(eventGroup);</span><br></pre></td></tr></table></figure></li><li><p>非主从Reactor多线程模式</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">EventLoopGroup eventGroup = <span class="keyword">new</span> NioEventLoopGroup();</span><br><span class="line">ServerBootstrap serverBootstrap = <span class="keyword">new</span> ServerBootstrap();</span><br><span class="line">serverBootstrap.group(eventGroup);</span><br></pre></td></tr></table></figure></li><li><p>主从 Reactor 多线程模式</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">EventLoopGroup bossGroup = <span class="keyword">new</span> NioEventLoopGroup();</span><br><span class="line">EventLoopGroup workerGroup = <span class="keyword">new</span> NioEventLoopGroup();</span><br><span class="line"></span><br><span class="line">ServerBootstrap serverBootstrap = <span class="keyword">new</span> ServerBootstrap();</span><br><span class="line">serverBootstrap.group(boosGroup, workerGroup);</span><br></pre></td></tr></table></figure></li></ul><h2 id="为什么TCP应用中会出现粘包和半包现象？"><a href="#为什么TCP应用中会出现粘包和半包现象？" class="headerlink" title="为什么TCP应用中会出现粘包和半包现象？"></a>为什么TCP应用中会出现粘包和半包现象？</h2><p>出现粘包现象的主要原因：</p><ul><li>发送方每次写入数据 &lt; &lt; 套接字缓冲区大小</li><li>接收方读取套接字缓冲区数据不够及时<br>以上二个原因可能会导致TCP接收到的数据出现粘包现象； </li></ul><p>出现半包现象的主要原因：</p><ul><li>发送方写入的数据 &gt; &gt; 套接字缓冲区大小 </li><li>发送的数据大于协议的MTU（Maximun Transmission Unit, 最大传输单元）, 必须拆包处理</li><li>IPV4 mtu 68, 64KiB</li><li>IPV6 mtu (1280, 64KiB) but up to 4GiB with optional</li><li>Ethernet 1500</li></ul><p>出现上述问题，原因： <strong>TCP是流式协议，消息无边界</strong><br>提醒： UDP像邮寄的包裹， 虽然一次运输多个， 但每个包裹都有“界限”， 一个一个签收， 所以无粘包， 半包问题。 </p><h2 id="解决问题的根本手段：-找出消息的边界："><a href="#解决问题的根本手段：-找出消息的边界：" class="headerlink" title="解决问题的根本手段： 找出消息的边界："></a>解决问题的根本手段： 找出消息的边界：</h2><p><img src="https://www.ant-loiter.com/img/tcp_package.png" alt=""></p><h2 id="封装成帧"><a href="#封装成帧" class="headerlink" title="封装成帧"></a>封装成帧</h2><ul><li>固定长度 FixedLengthFrameDecoder   easy</li><li>分割符 DelimiterBaseFrameDecoder   easy</li><li>固定长度字段存内容的长度信息 LengthFieldBaseFrameDecoder   LengthFieldPrepender </li></ul><h2 id="Netty-为什么需要“二次”-编解码？"><a href="#Netty-为什么需要“二次”-编解码？" class="headerlink" title="Netty 为什么需要“二次” 编解码？"></a>Netty 为什么需要“二次” 编解码？</h2><ul><li>一次解码器： ByteToMessageDecoder <ul><li>io.netty.buffer.ByteBuf (原始数据流) -&gt;  io.netty.buffer.ByteBuf （用户数据）</li></ul></li><li>二次解码器： MessageToMessageDecoder<t><ul><li>io.netty.buffer.ByteBuf(用户数据) -&gt; java Object </li></ul></t></li></ul><h2 id="常用-“-二次-”-编解码方式"><a href="#常用-“-二次-”-编解码方式" class="headerlink" title="常用 “ 二次 ” 编解码方式"></a>常用 “ 二次 ” 编解码方式</h2><p>java 序列化 &lt;  Marshaling &lt; XML &lt; JSON &lt; MessagePack &lt; Google Protobuf &lt; …</p><ul><li><p>Protobuf 是一个灵活的， 高效的用于序列化的数据协议； </p></li><li><p>相比较XML 和 JSON 格式， Protobuf 更小， 更快， 更便捷。 </p></li><li><p>Protobuf是跨语言的， 并具自带了一个编译器（Protoc)， 只需要用它进行编译， 可以自动生成Java, python, C++等代码， 不需要再写其他代码。 </p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;作者&quot;&gt;&lt;a href=&quot;#作者&quot; class=&quot;headerlink&quot; title=&quot;作者&quot;&gt;&lt;/a&gt;作者&lt;/h1&gt;&lt;p&gt;  Netty 由 Trustin Lee (韩国， Line公司) 2004年开发&lt;/p&gt;
&lt;h1 id=&quot;概述&quot;&gt;&lt;a href=&quot;#概
      
    
    </summary>
    
      <category term="Netty NetWork NetWork Protocol" scheme="https://www.ant-loiter.com/categories/Netty-NetWork-NetWork-Protocol/"/>
    
    
      <category term="Netty NetWork NetWork Protocol" scheme="https://www.ant-loiter.com/tags/Netty-NetWork-NetWork-Protocol/"/>
    
  </entry>
  
  <entry>
    <title>MySQL8.0.20_Master_Slave_replicationConfig</title>
    <link href="https://www.ant-loiter.com/2020/06/19/MySQL8-0-20-Master-Slave-replicationConfig/"/>
    <id>https://www.ant-loiter.com/2020/06/19/MySQL8-0-20-Master-Slave-replicationConfig/</id>
    <published>2020-06-19T10:23:46.000Z</published>
    <updated>2020-10-30T13:49:33.119Z</updated>
    
    <content type="html"><![CDATA[<h1 id="MySQL8-0-20-Master-Slave-Config"><a href="#MySQL8-0-20-Master-Slave-Config" class="headerlink" title="MySQL8.0.20 Master Slave Config"></a>MySQL8.0.20 Master Slave Config</h1><h2 id="master-service"><a href="#master-service" class="headerlink" title="master service"></a>master service</h2><p>open my.cnf config :<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">server-id = 1</span><br><span class="line">binlog_expire_logs_seconds=259200</span><br><span class="line">binlog-format=MIXED</span><br><span class="line">log-bin=mysql-bin</span><br><span class="line"></span><br><span class="line"># mysql8 startup failed </span><br><span class="line">default-time-zone=&apos;+8:00&apos;</span><br></pre></td></tr></table></figure></p><h2 id="slave-service"><a href="#slave-service" class="headerlink" title="slave service"></a>slave service</h2><p>open my.cnf config :<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">server-id = 2</span><br><span class="line">binlog-format=MIXED</span><br><span class="line">log-bin=mysql-bin</span><br><span class="line">binlog_expire_logs_seconds=259200</span><br><span class="line">log-slave-updates=1</span><br></pre></td></tr></table></figure></p><h2 id="look-master-service-binlog-flag"><a href="#look-master-service-binlog-flag" class="headerlink" title="look master service binlog flag"></a>look master service binlog flag</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">show master status;</span><br></pre></td></tr></table></figure><p>get binlog filename and binlog position </p><p><code>`</code></p><h1 id="create-repl-user"><a href="#create-repl-user" class="headerlink" title="create repl user"></a>create repl user</h1><p>create user ‘repl‘@’ip’ identified by ‘password’;<br>grant replication slave on <em>.</em> to ‘repl‘@’ip’;</p><p>not use this SQL:<br>grant replication slave on <em>.</em> to ‘repl‘@’IP’ identified by ‘password’;</p><h2 id="slave-service-1"><a href="#slave-service-1" class="headerlink" title="slave service"></a>slave service</h2><p>change master to<br>master_host=’ip’,<br>master_port=port,<br>master_user=’repl’,<br>master_password=’password’,<br>master_log_file=’master find binlog filename’,<br>master_log_pos=postition;</p><p>up command master_log_file &amp; master_log_pos values find by master server; </p><p>stop slave ;<br>start slave;<br>reset slave;</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;MySQL8-0-20-Master-Slave-Config&quot;&gt;&lt;a href=&quot;#MySQL8-0-20-Master-Slave-Config&quot; class=&quot;headerlink&quot; title=&quot;MySQL8.0.20 Master Slave Confi
      
    
    </summary>
    
      <category term="Master Slave Backup Restore" scheme="https://www.ant-loiter.com/categories/Master-Slave-Backup-Restore/"/>
    
    
      <category term="Master Slave Backup Restore" scheme="https://www.ant-loiter.com/tags/Master-Slave-Backup-Restore/"/>
    
  </entry>
  
  <entry>
    <title>SpringFormwork001</title>
    <link href="https://www.ant-loiter.com/2020/06/18/SpringFormwork001/"/>
    <id>https://www.ant-loiter.com/2020/06/18/SpringFormwork001/</id>
    <published>2020-06-18T09:31:17.000Z</published>
    <updated>2020-10-30T13:49:33.119Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Spring-模块"><a href="#Spring-模块" class="headerlink" title="Spring 模块"></a>Spring 模块</h1><p>spring FrameWork的模块化与java 9的模块化有所不同； </p><h1 id="Spring-模块化设计（Modular）"><a href="#Spring-模块化设计（Modular）" class="headerlink" title="Spring 模块化设计（Modular）"></a>Spring 模块化设计（Modular）</h1><table><thead><tr><th style="text-align:center">序号</th><th style="text-align:center">模块名</th><th style="text-align:center">描述说明</th><th style="text-align:center">序号</th><th style="text-align:center">模块名</th><th style="text-align:center">描述说明</th></tr></thead><tbody><tr><td style="text-align:center">1</td><td style="text-align:center">spring-aop</td><td style="text-align:center"></td><td style="text-align:center">2</td><td style="text-align:center">spring-jms</td><td style="text-align:center"></td></tr><tr><td style="text-align:center">3</td><td style="text-align:center">spring-aspects</td><td style="text-align:center"></td><td style="text-align:center">4</td><td style="text-align:center">spring-messaging</td><td style="text-align:center">统一消息的实现如jms，kafka等MQ</td></tr><tr><td style="text-align:center">5</td><td style="text-align:center">spring-context-indexer</td><td style="text-align:center"></td><td style="text-align:center">6</td><td style="text-align:center">spring-orm</td><td style="text-align:center"></td></tr><tr><td style="text-align:center">7</td><td style="text-align:center">spring-context-support</td><td style="text-align:center"></td><td style="text-align:center">8</td><td style="text-align:center">spring-oxm</td><td style="text-align:center">xml的序列化反序列化的模块</td></tr><tr><td style="text-align:center">9</td><td style="text-align:center">spring-context</td><td style="text-align:center">与spring-beans合成实现了IOC的功能,通地这core模块支持</td><td style="text-align:center">10</td><td style="text-align:center">spring-text</td><td style="text-align:center"></td></tr><tr><td style="text-align:center">11</td><td style="text-align:center">spring-core</td><td style="text-align:center"></td><td style="text-align:center">12</td><td style="text-align:center">spring-tx</td><td style="text-align:center">spring的事物抽象</td></tr><tr><td style="text-align:center">13</td><td style="text-align:center">spring-expression</td><td style="text-align:center"></td><td style="text-align:center">14</td><td style="text-align:center">spring-web</td><td style="text-align:center"></td></tr><tr><td style="text-align:center">15</td><td style="text-align:center">spring-instrument</td><td style="text-align:center"></td><td style="text-align:center">16</td><td style="text-align:center">spring-webflux</td><td style="text-align:center"></td></tr><tr><td style="text-align:center">17</td><td style="text-align:center">spring-jcl</td><td style="text-align:center">新模块从spring5开始支持的是spring引入的一个新型的日志框架spring以前就是通过comment-loging来支持的</td><td style="text-align:center">18</td><td style="text-align:center">spring-webmvc</td><td style="text-align:center"></td></tr><tr><td style="text-align:center">19</td><td style="text-align:center">spring-jdbc</td><td style="text-align:center"></td><td style="text-align:center">20</td><td style="text-align:center">spring-websocket</td></tr></tbody></table><p>change master to master_host=’192.168.1.212’,master_user=’repl1’,master_password=‘Test@123’,master_log_file=’mysql-bin.000002’,master_log_pos=443;</p><h2 id="资源"><a href="#资源" class="headerlink" title="资源"></a>资源</h2><ul><li>jsp 官方网址： <a href="https://jcp.org/" target="_blank" rel="noopener">https://jcp.org/</a></li><li>小马哥JSR收藏： <a href="https://github.com/mercyblitz/jsr" target="_blank" rel="noopener">https://github.com/mercyblitz/jsr</a></li><li>Spring官方文档根路径： <a href="https://docs.spring.io/spring/docs" target="_blank" rel="noopener">https://docs.spring.io/spring/docs</a><br><a href="https://docs.spring.io/spring-boot/docs" target="_blank" rel="noopener">https://docs.spring.io/spring-boot/docs</a></li></ul><h2 id="Spring编程模型"><a href="#Spring编程模型" class="headerlink" title="Spring编程模型"></a>Spring编程模型</h2><ul><li>面向对象编程</li><li>面向切面编程</li><li>面向元编程</li><li>函数驱动</li><li>模块驱动</li></ul><h2 id="IOC-容器的职责"><a href="#IOC-容器的职责" class="headerlink" title="IOC 容器的职责"></a>IOC 容器的职责</h2><ul><li>通用职责</li><li>依赖处理<ul><li>依赖查找</li><li>依赖注入</li></ul></li><li>生命周期管理<ul><li>容器</li><li>托管的资源（Java Beans或其他资源）</li></ul></li><li>配置<ul><li>容器</li><li>外部化配置</li><li>托管的资源（Java Beans 或 其他资源）</li></ul></li></ul><h2 id="IOC容器的实现"><a href="#IOC容器的实现" class="headerlink" title="IOC容器的实现"></a>IOC容器的实现</h2><ul><li>主要实现</li><li>Java SE<ul><li>Java Beans</li><li>Java ServiceLoader SPI</li><li>JNDI （Java Naming and Directory Interface）</li></ul></li><li>Java EE<ul><li>EJB （Enterprise Java Beans）</li><li>Servlet</li></ul></li><li>开源<ul><li>Apache Avalon （<a href="http://avalon.apache.org/closed.html" target="_blank" rel="noopener">http://avalon.apache.org/closed.html</a>)</li><li>PicoContainer (<a href="http://picocontainer.com/" target="_blank" rel="noopener">http://picocontainer.com/</a>)</li><li>Google Guice (<a href="https://github.com/google/guice" target="_blank" rel="noopener">https://github.com/google/guice</a>)</li><li>Spring Framework (<a href="https://spring.io/projects/spring-framework" target="_blank" rel="noopener">https://spring.io/projects/spring-framework</a>)  * </li></ul></li></ul><h2 id="依赖查找-VS-依赖注入"><a href="#依赖查找-VS-依赖注入" class="headerlink" title="依赖查找 VS 依赖注入"></a>依赖查找 VS 依赖注入</h2><table><thead><tr><th style="text-align:center">类型</th><th style="text-align:center">依赖处理</th><th style="text-align:center">实现便利性</th><th style="text-align:center">代码侵入性</th><th style="text-align:center">API依赖性</th><th style="text-align:center">可读性</th></tr></thead><tbody><tr><td style="text-align:center">依赖查找</td><td style="text-align:center">主动获取</td><td style="text-align:center">查对管繁琐</td><td style="text-align:center">侵入业务逻辑</td><td style="text-align:center">依赖容器API</td><td style="text-align:center">良好</td></tr><tr><td style="text-align:center">依赖注入</td><td style="text-align:center">被动提供</td><td style="text-align:center">相对便利</td><td style="text-align:center">低侵入性</td><td style="text-align:center">不依赖容器API</td><td style="text-align:center">一般</td></tr></tbody></table><h2 id="沙雕面试题-—-什么时IoC？"><a href="#沙雕面试题-—-什么时IoC？" class="headerlink" title="沙雕面试题  — 什么时IoC？"></a>沙雕面试题  — 什么时IoC？</h2><p> – 简单地说， IoC 是反转控制， 类似于好菜坞原则， 主要是依赖查找和依赖注入实现。</p><h2 id="996-面试题–-依赖查询和依赖注入的区别？"><a href="#996-面试题–-依赖查询和依赖注入的区别？" class="headerlink" title="996 面试题– 依赖查询和依赖注入的区别？"></a>996 面试题– 依赖查询和依赖注入的区别？</h2><p>  — 依赖查找是主动或手机依赖查询方式， 通常需要依赖容器或标准API实现，如依赖名称， 资源等方式获得对象信息，servlet API， EJB API， JNDI API； 而依赖注入则是手动或自动依赖绑定的方式， 无需依赖特定的容器和API。 </p><h2 id="劝退面试题-–-Spring-作为IoC-容器有什么优势？？"><a href="#劝退面试题-–-Spring-作为IoC-容器有什么优势？？" class="headerlink" title="劝退面试题 – Spring 作为IoC 容器有什么优势？？"></a>劝退面试题 – Spring 作为IoC 容器有什么优势？？</h2><p>  – 典型的IoC 管理， 依赖查询的依赖注入<br>  AOP抽象<br>  事务抽象<br>  事机机制<br>  SPI扩展<br>  强大的第三方整合<br>  易测试性<br>  更好的面向对象</p><h2 id="Spring-IoC-依赖查找"><a href="#Spring-IoC-依赖查找" class="headerlink" title="Spring IoC 依赖查找"></a>Spring IoC 依赖查找</h2><ul><li>根据Bean 名称查询<ul><li>实时查找</li><li>延迟查找</li></ul></li><li>根据Bean 类型查找<ul><li>单个Bean 对象</li><li>集合 Bean 对象</li></ul></li><li>根据Bean 名称 + 类型查找</li><li>根据Java 注解查找<ul><li>单个 Bean 对象</li><li>集合 Bean 对象</li></ul></li></ul><h2 id="Bean-的来源"><a href="#Bean-的来源" class="headerlink" title="Bean 的来源"></a>Bean 的来源</h2><ul><li>自建的Bean， 并通过配置产生Bean实例</li><li>依赖的Bean， 容器自己自动注入</li><li>非Bean ， 内部容器所构建的Bean</li></ul><h2 id="Spring-IoC-配置元信息"><a href="#Spring-IoC-配置元信息" class="headerlink" title="Spring IoC 配置元信息"></a>Spring IoC 配置元信息</h2><ul><li>Bean 定义配置<ul><li>基于XML 文件</li><li>基于Properties文件</li><li>基于Java 注解</li><li>基于Java API（专题讨论）</li></ul></li><li>IoC 容器配置<ul><li>基于XML文件</li><li>基于Java注解</li><li>基于Java API（专题讨论）</li></ul></li><li>外部化属性配置<ul><li>基于Java注解</li></ul></li></ul><h2 id="BeanFactory-与-ApplicationContext-倒底哪个是IoC的主要特征体理？"><a href="#BeanFactory-与-ApplicationContext-倒底哪个是IoC的主要特征体理？" class="headerlink" title="BeanFactory 与 ApplicationContext 倒底哪个是IoC的主要特征体理？"></a>BeanFactory 与 ApplicationContext 倒底哪个是IoC的主要特征体理？</h2><ul><li>ApplicationContext 不仅是BeanFactory的超类， 而且还是支持很多Spring 其它特性如：<ul><li>Events</li><li>i18n</li><li>Annotations</li><li>AOP</li><li>Resources</li><li>Configuration Metadata</li><li>Environment 抽象（Environment Abstraction)<br>在ApplicationContext中可以获取到原生功能的BeanFactory; </li></ul></li></ul><p><strong>技巧：<em>shift + fn + f6 可以重构变量名</em></strong></p><h2 id="ApplicationContext-上下文对象的关键方法："><a href="#ApplicationContext-上下文对象的关键方法：" class="headerlink" title="ApplicationContext 上下文对象的关键方法："></a>ApplicationContext 上下文对象的关键方法：</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 启动应用上下文</span></span><br><span class="line">applicationContext.refresh();</span><br><span class="line"><span class="comment">// 这一步， 完成上下文的处理， 同时初始化了很多非bean对象， 如支持事件， 支持广播， 支持国际化， 支持注解等； </span></span><br><span class="line">perareBeanFactory(beanFactory);</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>&#123;</span><br><span class="line"></span><br><span class="line">&#125; <span class="keyword">catch</span>(Exception e)&#123;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">applicationContext.close();</span><br></pre></td></tr></table></figure><h3 id="什么时Spring-IoC-容器？"><a href="#什么时Spring-IoC-容器？" class="headerlink" title="什么时Spring IoC 容器？"></a>什么时Spring IoC 容器？</h3><p>Spring Framework implementation of the Inversion(反转) of Control(IoC)principle.<br>IoC is also known as dependency injection（注入）(DI). It is a process where by objects define their dependencies(that is , the other objects they work with) only through constructor arguments, arguments to a factory method, or properties that are set on the object instance after it is constructed or returned from a factory method. The container then injects those dependencies when it creates the bean.</p><p>依赖查找， 其实在早期的javaEE实现了。 </p><h3 id="BeanFactory-与-FactoryBean"><a href="#BeanFactory-与-FactoryBean" class="headerlink" title="BeanFactory 与 FactoryBean ?"></a>BeanFactory 与 FactoryBean ?</h3><p>BeanFactory 是IoC的底层容器。<br>FactoryBean 是创建Bean的一种方式， 帮助实现复杂的初始化逻辑。 </p><h3 id="Spring-IoC容器启动时做了哪些准备？"><a href="#Spring-IoC容器启动时做了哪些准备？" class="headerlink" title="Spring IoC容器启动时做了哪些准备？"></a>Spring IoC容器启动时做了哪些准备？</h3><p>IoC 配置元信息读取和解析、IoC容器生命周期、Spring事件发布、国际化等； </p><h3 id="实例化-Spring-Bean"><a href="#实例化-Spring-Bean" class="headerlink" title="实例化 Spring Bean"></a>实例化 Spring Bean</h3><ul><li><p>Bean 实例化（Instantiation)</p><ul><li>常规方式<ul><li>通过构造器（配置元信息： XML、 Java注解和Java API）</li><li>通过静态工厂方法（ 配置元信息： XML 和 Java API）</li><li>通过Bean 工厂方法（配置元信息， XML 和 Java API）</li><li>通过 FactoryBean (配置元信息： XML 、 java 注解 和 Java API)</li></ul></li><li>特殊方式<ul><li>通过ServiceLoaderFactoryBean （配置元信息： XML、 java注解和Java API）</li><li>通过AutowireCapaleBeanFactory#ceateBean（java.lang.Class,int , boolean)</li><li>通过BeanDefinitaionRegistry#registerBeanDefinition(String, BeanDefinition) </li></ul></li></ul></li></ul><h3 id="Spring-Bean延迟初始化"><a href="#Spring-Bean延迟初始化" class="headerlink" title="Spring Bean延迟初始化"></a>Spring Bean延迟初始化</h3><ul><li><p>Bean 延迟初始化（lazy Initialization)</p><ul><li>XML 配置： &lt;bean lazy-init=”true”…/&gt;</li><li>java 注解： @Lazy(true)<br>Java注解是个静态化的东西， 一旦定义就不会修改。 <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Lazy</span>(value = <span class="keyword">false</span>)</span><br><span class="line"><span class="comment">// 以上注解建议不打</span></span><br></pre></td></tr></table></figure></li></ul></li><li><p>非延迟初始化在spring应用上下文启动完成后， 被初始化 </p></li><li>延迟初始化，其实就是按需进行初始化。</li><li>在applicationContext中refresh()方法源码中， 有一个方法<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Instantiate all remaing (non-lazy-init) singletons</span></span><br><span class="line">finishBeanFactoryInitialization(beanFactory);  </span><br><span class="line"><span class="comment">// 这个方法执行完后， 就会初始化所有非延迟初始化的bean </span></span><br><span class="line"></span><br><span class="line">在应用上下文初始化前后进行输出。</span><br></pre></td></tr></table></figure></li></ul><h3 id="Spring-Bean-销毁"><a href="#Spring-Bean-销毁" class="headerlink" title="Spring Bean 销毁"></a>Spring Bean 销毁</h3><ul><li>Bean 销毁（Destroy)<ul><li>@PreDestroy 标注方法</li><li>实现DisposableBean 接口的 destory() 方法</li><li>自定义销毁方法<ul><li>XML 配置： <bean destroy="destroy" ..=""></bean></li><li>Java注解： @Bean(destory = “destory”)</li><li>Java API: AastractBeanDefinition#setDestroyMethodName(String)</li></ul></li></ul></li></ul><h3 id="Spring-Bean-垃圾回收"><a href="#Spring-Bean-垃圾回收" class="headerlink" title="Spring Bean 垃圾回收"></a>Spring Bean 垃圾回收</h3><ul><li>Bean 垃圾回收（GC）<ul><li>关闭Spring 容器（应用上下文）</li><li>执行GC</li><li>Spring Bean 覆盖finalize() 方法被回调 </li></ul></li></ul><h3 id="依赖查找的今世前生"><a href="#依赖查找的今世前生" class="headerlink" title="依赖查找的今世前生"></a>依赖查找的今世前生</h3><ul><li>单一类型依赖查找<ul><li>JNDI - javax。naming。Context#lookup（javax.naming.Name)</li><li>JavaBeans - java.beans.beancontext.BeanContext</li></ul></li><li>集合类型依赖查找<ul><li>java.beans.beancontext.BeanContext</li></ul></li><li>层次性依赖查找<ul><li>java.beans.beancontext.BeanContext</li></ul></li></ul><p>*<em>第45集完全没有听懂</em></p><p><strong>Spring是不支持点对点的广播</strong></p><h2 id="ObjectFactory-与-BeanFactory-的区别？"><a href="#ObjectFactory-与-BeanFactory-的区别？" class="headerlink" title="ObjectFactory 与 BeanFactory 的区别？"></a>ObjectFactory 与 BeanFactory 的区别？</h2><ul><li>ObjectFactory 与 BeanFactory 均提供依赖查找的能力。<br>不过 ObjectFactory 仅关注一个或一种类型的Bean依赖查找， 并且自身不具备依赖查找的能力， 能力则由BeanFactory输出。</li></ul><p>BeanFactory则提供了单一类型、集合类型以及层次性等多种依赖查找方式； </p><h2 id="BeanFactory-getBean操作是否线程安全？"><a href="#BeanFactory-getBean操作是否线程安全？" class="headerlink" title="BeanFactory.getBean操作是否线程安全？"></a>BeanFactory.getBean操作是否线程安全？</h2><p>BeanFactory.getBean方法的执行是线程安全的， 操作过程中会增加互斥锁； 在jdk6 出现偏向锁后， 一定程度上减少了锁的竞争，提升了加锁的效率 ； </p><h2 id="字段注入"><a href="#字段注入" class="headerlink" title="字段注入"></a>字段注入</h2><ul><li>实现方法<ul><li>手动模式<ul><li>Java注解配置元信息<ul><li>@Autowired  会忽略掉static 静态字段。 实际是只能注入实例字段或叫对象字段。 不能注入类的静态字段， 会自动忽略掉所有static字段。 </li><li>@Resource</li><li>@Inject (可选)</li></ul></li></ul></li></ul></li></ul><h2 id="接口回调注入"><a href="#接口回调注入" class="headerlink" title="接口回调注入"></a>接口回调注入</h2><ul><li>Aware 系列接口回调<ul><li>自动模式<br>|内建接口| 说明 |<br>|:—:|:—|<br>|BeanFactoryAware|获取IoC容器 - BeanFactory |<br>|ApplicationContextAware|获取Spring 应用上下文-ApplicationContext对象|<br>|EnvironmentAware| 获取Environment对象|<br>|ResourceLoaderAware|获取资源加载器对象- ResoueceLoader|<br>|BeanClassLoaderAware|获取加载当前Bean Class 的 ClassLoader|<br>|BeanNameAware|获取当前Bean的名称| </li></ul></li></ul><h2 id="依赖注入类型选择"><a href="#依赖注入类型选择" class="headerlink" title="依赖注入类型选择"></a>依赖注入类型选择</h2><ul><li>注入造型<ul><li>低依赖： 构造器注入  byType*  byName  官方推荐</li><li>多依赖： Setter方法注入  有个不足， 就是注入时先后顺序依赖于用户操作先后顺序，如果有注入的字段或属性有顺序要求， 这个方法注入可能会有些问题。  </li><li>便利性： 字段注入 </li><li>声明类： 方法注入 </li></ul></li></ul><h2 id="基础类型注入"><a href="#基础类型注入" class="headerlink" title="基础类型注入"></a>基础类型注入</h2><ul><li>基础类型<ul><li>原生类型（Primitive）： boolean、byte 、 char 、 short、int、float、long、double</li><li>标量类型（Scalar)： Number、Character、Boolean、 Enum、Locale、Charset、Currency、Properties、UUID</li><li>常规类型(General): Object、String、TimeZone、Calendar、Optional</li><li>Spring类型： Resource、InputSource、Formatter</li></ul></li></ul><h2 id="集合类型注入"><a href="#集合类型注入" class="headerlink" title="集合类型注入"></a>集合类型注入</h2><ul><li><p>集合类型</p><ul><li><p>数组类型（Array）： 原生类型、标量类型、 常规类型、Spring类型</p></li><li><p>集合类型（Collection）</p><ul><li>Collection： List、Set、(Sortedset、NavigableSet、EnumSet)</li><li>Map: Properties</li></ul></li></ul></li></ul><h2 id="限定注入"><a href="#限定注入" class="headerlink" title="限定注入"></a>限定注入</h2><ul><li>使用注解 @Qualifier 限定<ul><li>通过Bean 名称限定</li><li>通过分组限定</li></ul></li><li>基于注解 @Qualifier 扩展限定<ul><li>自定义注解 - 如 Spring Cloud @LoadBalanced </li></ul></li></ul><h2 id="延迟依赖注入"><a href="#延迟依赖注入" class="headerlink" title="延迟依赖注入"></a>延迟依赖注入</h2><ul><li>使用 API ObjectFactory 延迟注入<ul><li>单一类型</li><li>集合类型</li></ul></li><li>使用API Objectprovider 延迟注入 （推荐）， 其实就是扩展了ObjectFactory的类<ul><li>单一类型</li><li>集合类型</li></ul></li></ul><h2 id="依赖处理过程"><a href="#依赖处理过程" class="headerlink" title="依赖处理过程"></a>依赖处理过程</h2><ul><li>基础知识<ul><li>入口 - DefaultListableBeanFactory # resolveDependency</li><li>依赖描述符： DependencyDescriptor</li><li>自定绑定候选处理器 - AutowireCandidateResolver </li></ul></li></ul><h2 id="Autowired-注入"><a href="#Autowired-注入" class="headerlink" title="@Autowired 注入"></a>@Autowired 注入</h2><ul><li>@Autowired 注入过程<ul><li>元信息解析</li><li>依赖查找</li><li>依赖注入（ 字段、 方法） </li></ul></li><li>核心类 AutowiredAnnotationBeanPostProcessor<ul><li>postProcessMergedBeanDefinition</li><li>postProcessProperties</li></ul></li></ul><h2 id="Inject-注入"><a href="#Inject-注入" class="headerlink" title="@Inject 注入"></a>@Inject 注入</h2><ul><li><p>@Inject 注入过程</p><ul><li>如果JSR-330 存在于ClassPath 中， 复用AutowiredAnnotationBeanPostProcessor 实现。 </li><li>可以处理 @Autowired @Value @Inject</li><li>AutowiredAnnotationBeanPostProcessor 进行了 @Autowired 和 @Inject 的注解处理。 处理逻辑基本一致。</li></ul></li><li><p>CommonAnnotationBeanPostProcessor </p><ul><li>注入注解<ul><li>javax.xml.ws.WebServiceRef</li><li>javax.ejb.EJB</li><li>javax.annotation.Resource</li></ul></li><li>生命周期注解<ul><li>javax.annotation.PostConstruct</li><li>javax.annotation.PreDestroy</li></ul></li></ul></li></ul><h2 id="如何自定义依赖注入"><a href="#如何自定义依赖注入" class="headerlink" title="如何自定义依赖注入"></a>如何自定义依赖注入</h2><ul><li><p>基于AutowiredAnnotationBeanPostProcessor 实现</p></li><li><p>自定义实现</p><ul><li>生命周期处理<ul><li>InstanticationAwareBeanPostProcessor</li><li>MergedBeanDefinitionPostProcessor</li></ul></li><li>元数据<ul><li>InjectedElement</li><li>InjectionMetadata</li></ul></li></ul></li></ul><h3 id="有多少种依赖注入的方式？"><a href="#有多少种依赖注入的方式？" class="headerlink" title="有多少种依赖注入的方式？"></a>有多少种依赖注入的方式？</h3><ul><li>构造器注入 ， 少依赖， 强制依赖的情况下</li><li>Setter 注入， 多依赖， 弱依赖</li><li>字段注入   开发比较便利</li><li>方法注入   @Bean 方便声明， </li><li>接口回调注入   比较特殊的注入方法， 一般可以做一些别的事情， 如生命周期式的。 </li></ul><h3 id="你偏好构造器注入还是Setter注入？"><a href="#你偏好构造器注入还是Setter注入？" class="headerlink" title="你偏好构造器注入还是Setter注入？"></a>你偏好构造器注入还是Setter注入？</h3><p>  两种依赖注入的方式均可使用， 如果是必须依赖的话， 那么推荐使用构造器注入， Setter注入用于可选依赖。<br>  少参数， 建议首先构造器注入， </p><h3 id="Spring-依赖注入的来源有哪些？"><a href="#Spring-依赖注入的来源有哪些？" class="headerlink" title="Spring 依赖注入的来源有哪些？"></a>Spring 依赖注入的来源有哪些？</h3><ul><li>依赖注入是用 ResolverDependency </li><li>依赖查找是用 BeanFactory.getBean() </li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Spring-模块&quot;&gt;&lt;a href=&quot;#Spring-模块&quot; class=&quot;headerlink&quot; title=&quot;Spring 模块&quot;&gt;&lt;/a&gt;Spring 模块&lt;/h1&gt;&lt;p&gt;spring FrameWork的模块化与java 9的模块化有所不同； &lt;/p&gt;

      
    
    </summary>
    
      <category term="Spring Framework SpringBoot SpringCloud" scheme="https://www.ant-loiter.com/categories/Spring-Framework-SpringBoot-SpringCloud/"/>
    
    
      <category term="Spring Framework SpringBoot SpringCloud" scheme="https://www.ant-loiter.com/tags/Spring-Framework-SpringBoot-SpringCloud/"/>
    
  </entry>
  
  <entry>
    <title>distributed_ku_table</title>
    <link href="https://www.ant-loiter.com/2020/06/16/distributed-ku-table/"/>
    <id>https://www.ant-loiter.com/2020/06/16/distributed-ku-table/</id>
    <published>2020-06-16T11:21:03.000Z</published>
    <updated>2020-10-30T13:49:33.119Z</updated>
    
    <content type="html"><![CDATA[<p><strong>每一个优秀的程序员和架构师都应该掌握分库分表</strong></p><p>移动互联网时代， 海量的用户每天产生海量的数据，比如：</p><ul><li>用户表</li><li>订单表</li><li>交易流水表</li></ul><p>事实上MySQL单表可以存储10亿级数据， 只是这时候性能比较差， 业务公认MySQL单表容量在1KW以下是最佳状态， 因为这时它的BTree索引树高在3-5之间。 </p><p>即然一张表无法搞定， 那么就要想办法将数据放到多个地方， 目前比较普遍的方案有3个：</p><ul><li>表分区</li><li>分库分表</li><li>NoSQL/NewSQL, NoSQL代表MongoDB， ES； NewSQL代表TiDB</li></ul><p>RDBMS 系统有以下几个NoSQL和NewSQL无法比的</p><ul><li>RDBMS 生态完善</li><li>RDBMS 绝对稳定</li><li>RDBMS 事务特性ACID</li></ul><h2 id="分区"><a href="#分区" class="headerlink" title="分区"></a>分区</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">分区表是由多个相关底层表实现， 这些底层表也是由句柄对象表示， 所以我们也可以直接访问各个分区， 存储引擎管理分区的各个底层表和管理普通表一样（所有底层表都必须使用相同的存储引擎），分区表的索引只是在各有个底层表上各自加上一个相周的索引， 从存储引擎的角度来看， 底层表和一个普通表没有任何不同， 存储引擎也无须知道这是一个普通表还是一个分区表的一部分。</span><br></pre></td></tr></table></figure><p>这种方案有一个很大的问题就是还是只能在一个MySQL实例中， 所以IO是个很严重的问题。 但是如果上层缓存做的强大， 也可以接受。 </p><p>表分区无法使用外键， 不支持全文索引。 我认为这都不算什么缺点， 现在的项目如果还是大量使用外键和数据库全文索引， 也真是不应该。 有太多的技术可以实现的更好，数据库本身的全文索引本身就是一个普通功能。 </p><p>一般 来讲， 如果使用分区表， 业务应该要具备以下两个特点：</p><ul><li>数据不是海量（分区数有限， 存储能力有限）</li><li>并发能力要求不高</li></ul><h2 id="分库分表"><a href="#分库分表" class="headerlink" title="分库分表"></a>分库分表</h2><p>中间件：</p><ul><li>阿里的TDDL， DRDS 和 cobar</li><li>开源社区sharding-jdbc（3.x已经更名为sharding-sphere）</li><li>民间组织的MyCAT</li><li>360的Atlas</li><li>美团的zebra</li></ul><p>分库分表， 主要分二大类型：</p><ul><li>Client 模式； </li><li>Proxy 模式； </li></ul><p>Client模式代表有阿里的TDDL， 开源社区的sharding-jdbc（sharding-sphere3.x已经支持proxy模式了），架构如下：<br><img src="https://mmbiz.qpic.cn/mmbiz_jpg/Naw09lVL3GSRvyfFeLZbiagKUQEibmWiaLsn5ZBiboZumFUkVTgjGxfOjFrPko2c2uic98UAxX8icMg0FicqCTxykQsSw/640?wx_fmt=jpeg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt=""></p><p>Proxy模式代表有阿里的cobar ， 民间组织的MyCAT，架构如下：</p><p><img src="https://mmbiz.qpic.cn/mmbiz_jpg/Naw09lVL3GSRvyfFeLZbiagKUQEibmWiaLsf7qaknTDolvUyq5wQuapgDJojUnr89Uv7tnAJdhS3QUnUbPMNMXNmQ/640?wx_fmt=jpeg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt=""></p><p>但是，无论是CLIENt模式， 还是PROXY模式， 几个核心的步骤是一样的： SQL解析、重写、路由、执行，结果归并。 </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;每一个优秀的程序员和架构师都应该掌握分库分表&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;移动互联网时代， 海量的用户每天产生海量的数据，比如：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;用户表&lt;/li&gt;
&lt;li&gt;订单表&lt;/li&gt;
&lt;li&gt;交易流水表&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;事实上
      
    
    </summary>
    
      <category term="分库分表 实战" scheme="https://www.ant-loiter.com/categories/%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8-%E5%AE%9E%E6%88%98/"/>
    
    
      <category term="分库分表 实战" scheme="https://www.ant-loiter.com/tags/%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8-%E5%AE%9E%E6%88%98/"/>
    
  </entry>
  
  <entry>
    <title>Redis-distributed-Lock</title>
    <link href="https://www.ant-loiter.com/2020/06/16/Redis-distributed-Lock/"/>
    <id>https://www.ant-loiter.com/2020/06/16/Redis-distributed-Lock/</id>
    <published>2020-06-16T02:53:10.000Z</published>
    <updated>2020-10-30T13:49:33.119Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Redis-Distributed-Lock-实现"><a href="#Redis-Distributed-Lock-实现" class="headerlink" title="Redis Distributed Lock 实现"></a>Redis Distributed Lock 实现</h1><h2 id="分布式概览"><a href="#分布式概览" class="headerlink" title="分布式概览"></a>分布式概览</h2><p>在多线程环境下， 为了保证一个代码在同一时间只能由一个多线程， java中我们一般可以使用synchronized 语法和ReetrantLock去保证， 这实际上是本地锁的方式。 但是现在都是流行分布式的加构， 在分布式的环境下， 如何保证不同节点的线程同步执行？？</p><p>实际上， 对于分布式场景， 可以使用分布式锁， 它是用来控制分布式系统之间互斥访问共享资源的一种方式。 </p><p>比如说在一个分布式系统中， 多台机器上部署了多个服务， 当客户端一个用户发起一个数据插请求时， 如果没有分布式锁机制保证， 那么这多台机器上的多个服务可能进行并发插入操作， 导致数据重复插入， 对于某些不允许有多余数据的业务来说， 这就是会造成问题。 而分布式锁机制就是为了解决类似这类问题。 保证多个服务之间互斥的访问共享资源， 如果一个服务抢占了分布式锁， 共他服务 没有获取到锁， 就不进行后续操作， 如下图所示：<br><img src="https://www.ant-loiter.com/img/distributed-lock.png" alt="Distributed Lock"></p><h2 id="分布式锁的特点"><a href="#分布式锁的特点" class="headerlink" title="分布式锁的特点"></a>分布式锁的特点</h2><p>分布式锁一般有如下的特点：</p><ul><li>互斥性， 同一时刻只能一个线程特有锁</li><li>可重入性： 同一节点上的同一个线程如果获取了锁之后能够再次获取锁； </li><li>锁超时： 和J.U.C中的锁一样支持锁超时， 防止死锁</li><li>高性能和高可用： 加锁和解锁需要高效， 同时也需要保证高可用， 防止分布式锁失效； </li><li>具备阻塞和非阻塞性， 能够及时从阻塞状态中被唤醒</li></ul><h2 id="分布式锁的实现方式"><a href="#分布式锁的实现方式" class="headerlink" title="分布式锁的实现方式"></a>分布式锁的实现方式</h2><p>一般有以下几种方式：</p><ul><li><p>基于数据库</p></li><li><p>基于Redis</p></li><li><p>基于zookeeper</p></li></ul><h2 id="Redis分布式锁实现"><a href="#Redis分布式锁实现" class="headerlink" title="Redis分布式锁实现"></a>Redis分布式锁实现</h2><ul><li>利用setnx + expire命令（不严谨）</li></ul><p>Redis的SetNX命令， setnx key value, 将key设置为value, 当键不存在时， 才能成功， 若键成功， 什么也不做， 成功返回1， 失败返回0. SETNX实际上就是SET IF NOT Exists的缩写。 </p><p>因为分布式锁还需要有超时机制， 所以需要利用expire命令来设置超时时间， 否则可能会出现死锁。 代码段如下：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">tryLock</span><span class="params">(String key, String request, <span class="keyword">int</span> timeout)</span> </span>&#123;</span><br><span class="line">  Long result = jedis.setnx(key, request);</span><br><span class="line">  <span class="comment">// result == 1时， 设置成功， 否则设置失败</span></span><br><span class="line">  <span class="keyword">if</span>(result == <span class="number">1l</span>) &#123;</span><br><span class="line">    <span class="keyword">return</span> jedis.expire(key, timeout) == <span class="number">1l</span>;</span><br><span class="line">  &#125;<span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>实际情况，上面的二步操作setnx 和 expire 是不具备原子性的。 有可能会出现expire执行不成功的情况， 如果也会出现死锁。 </p><p>有一种改善方案就是使用lua脚本来保证其原子性（包含setnx 和 expire 两条指令）</p><ul><li>使用Lua脚本（包含setnx和expire两条指令）</li></ul><p>代码如下：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">tryLock_with_lua</span><span class="params">(String key, String UniqueId, <span class="keyword">int</span> seconds)</span> </span>&#123;</span><br><span class="line">  String lua_scripts = <span class="string">"if redis.call('setnx', KEYS[1], ARGV[1]) == 1 then"</span> + <span class="string">"redis.call('expire', KEYS[1], ARGV[2]) return 1 else returen 0 end"</span>;</span><br><span class="line"></span><br><span class="line">  List&lt;String&gt; keys = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">  List&lt;String&gt; values = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">  keys.add(key);</span><br><span class="line">  values.add(UniqueId);</span><br><span class="line">  values.add(String.valueOf(seconds));</span><br><span class="line">  Object result = jedis.eval(lua_scripts, keys, values);</span><br><span class="line">  <span class="comment">// 判断是否成功</span></span><br><span class="line">  <span class="keyword">return</span> result.equals(<span class="number">1l</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><ul><li>使用set key value [EX seconds][PX milliseconds][NX|XX] 命令</li></ul><p>在redis 2.6.12 开始， 为set 命令增加一系列选项：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">set key value[EX seconds][PX milliseconds][NX|XX]</span><br></pre></td></tr></table></figure><ul><li>EX seconds : 设定过期时间 ， 单位为秒</li><li>PX milliseconds: 设定过期时间 ，单位为毫秒</li><li>NX: 仅当key不存在时设置值</li><li>XX：仅当key存在时设置值<br>set 命令的nx选项， 就等于setnx 命令， 如下；<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">tryLock_with_set</span><span class="params">(String key, String UniueId, <span class="keyword">int</span> seconds)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">return</span> <span class="string">"OK"</span>.equals(jedis.set(key, UniueId, <span class="string">"NX"</span>, <span class="string">"EX"</span>, seconds));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul><p>value必须具有唯一性， 我们可以用UUID来做， 设置随机字符串来保证唯一性， 至于为什么要保证唯一性？ 假如value不是随机字符串， 而是一个固定值， 那么就可能存在下面的问题。</p><ul><li>客户1 获取了锁成功； </li><li>客户1在某个操作阻塞了较长时间</li><li>设置的key过期了， 锁自动释放了</li><li>客户2 获取到了对应同一个资源的锁</li><li>客户1从阻塞中恢复过来， 因为value值一样， 所以执行释放操作会释放掉客户端2持有的锁， 这样就会造成问题。 </li></ul><h3 id="释放锁的实现"><a href="#释放锁的实现" class="headerlink" title="释放锁的实现"></a>释放锁的实现</h3><p>释放锁时需要验证value值， 也就是说我们在获取锁的时候需要设置一个value， 不能直接用del key这种粗暴的方式， 因为直接del key任何客户端都可以进行解锁了， 所以解锁时，需要判断锁的值是否与自己拿的value一致。 代码如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">releaseLock_lua</span><span class="params">(String key, String value)</span> </span>&#123;</span><br><span class="line">  String lueScripts = <span class="string">"if redis.call('get', KEYS[1]) == ARGV[1] then "</span> + <span class="string">"return redis.call('del', KEYS[1]) else return 0 end;</span></span><br><span class="line"><span class="string">  return jedis.eval(luaScripts, Collections.singLetonList(key), Collections.singletonList(value)).equest(1l);</span></span><br><span class="line"><span class="string">&#125;</span></span><br></pre></td></tr></table></figure><p>使用lua脚本方式， 尽是保证其原子性。 </p><h1 id="注意："><a href="#注意：" class="headerlink" title="注意："></a>注意：</h1><p><strong>set key value [EX seconds][PX milliseconds][NX|XX]在redis的主备环境下也有可能出现问题，如果数据写入到master上还没有还得及同步到slave节点，master服务宕掉，slave节点成为了master这样就会出现锁丢失，针对这种情况，在Redis集群下还有其他方案</strong></p><ul><li><p>RedLock算法 及 Redisson 实现 </p><p>可以查看开源项目<br><a href="https://github.com/BrendaHub/spring-cloud-loiter-demos.git" target="_blank" rel="noopener">redis distributed lock</a></p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Redis-Distributed-Lock-实现&quot;&gt;&lt;a href=&quot;#Redis-Distributed-Lock-实现&quot; class=&quot;headerlink&quot; title=&quot;Redis Distributed Lock 实现&quot;&gt;&lt;/a&gt;Redis Distr
      
    
    </summary>
    
      <category term="Redis Distributed Lock Cache" scheme="https://www.ant-loiter.com/categories/Redis-Distributed-Lock-Cache/"/>
    
    
      <category term="Redis Distributed Lock Cache" scheme="https://www.ant-loiter.com/tags/Redis-Distributed-Lock-Cache/"/>
    
  </entry>
  
  <entry>
    <title>JavaEnterprise_Architect_001</title>
    <link href="https://www.ant-loiter.com/2020/06/14/JavaEnterprise-Architect-001/"/>
    <id>https://www.ant-loiter.com/2020/06/14/JavaEnterprise-Architect-001/</id>
    <published>2020-06-14T09:46:46.000Z</published>
    <updated>2020-10-30T13:49:33.118Z</updated>
    
    <content type="html"><![CDATA[<h1 id="分布式事务这深入理解？"><a href="#分布式事务这深入理解？" class="headerlink" title="分布式事务这深入理解？"></a>分布式事务这深入理解？</h1><p><em>2PC、3PC及TCC协议</em></p><h2 id="什么是分布式事务"><a href="#什么是分布式事务" class="headerlink" title="什么是分布式事务"></a>什么是分布式事务</h2><p>其实分布式事务从实质上看与数据库事务的概念是一致的， 既然是事务也就需要满足事务的基本特征（ACID）， 只是分布式事务相对于本地事务而言其表现形式有很大的不同， 如， 一个jvm进程中如果需要同时操作数据库的多条记录， 而这些操作需要在一个事务中，那么我们可以通过数据库的事务机制（一般的数据库锁）来实现。 </p><p>而随着这个jvm进程（应用）被拆分成了微服务架构， 原本一个本地逻辑执行单元被拆分到了多个独立的微服务中， 这些微服务又分别操作不同的数据库和表， 服务之间通过网络调用。 </p><p>如，服务A收到笔购物下单请求后， 需要调用服务B去支付， 支付 成功则处理购物订单为待发货状态， 否则就需要将购物订单处理为失败状态。</p><p>分布式事务实现方式有很多种， 最且有代表性的是由Oracle Tuxdeo系统提出的XA分布式事务协议， XA协议包括两阶段提交（2PC）和三阶段提交（3PC）两种理实， 接下来我们分别来介绍这两种实现方工的原理。 </p><h3 id="两阶段提交（2PC）"><a href="#两阶段提交（2PC）" class="headerlink" title="两阶段提交（2PC）"></a>两阶段提交（2PC）</h3><p>两阶段提交又称为2PC（two-phase commit protocol）， 2PC是一个非常经典的强一致、中心化的原子提交协议。 这里所说的中心化是指协议中有两类节点：一个是中心化的协调者节点（coordinator)和N<br>个参与者节点（Partcipant）。 </p><h4 id="第一阶段：-请求、表决阶段"><a href="#第一阶段：-请求、表决阶段" class="headerlink" title="第一阶段： 请求、表决阶段"></a>第一阶段： 请求、表决阶段</h4><p>Starter(分布式事务发起者) -&gt; （1、发起请求调用）-&gt; Coordinator 协调者 init </p><p>-&gt; Partcipant (参与者A) -&gt; 开启本地事务操作数据库</p><p>-&gt; Partcipant (参与者B) -&gt; 开启本地事务操作数据库</p><p>就是分布式事务的发起方在向分布式事务协调者（Coordinator）发送请求时， Coordinator首先会分别向参与者（Partcipant）节点A、参与这节点（Partcipant)节点B分别发送事务预处理请求，称之为Prepare, 有些资料也叫“Vote Request” 。 </p><p>说的直白点就是问一下这些参与节点”这件事情你们能不能处理成功了“ ， 此时参与者节点一般来说都会打开本地数据库事务， 然后开始执行数据本地事务， 但在执行完成后并不会立马提交数据库本地事务， 而先向Coordinator报告说，我这边可以处理了、我这边不能处理。 </p><p>如果所有的参与者与这个点向协调者作了”vote Commit“的反馈的话， 那么此时流程就会进入第二个阶段了。 </p><h4 id="第二个阶段，-提交-执行阶段"><a href="#第二个阶段，-提交-执行阶段" class="headerlink" title="第二个阶段， 提交/执行阶段"></a>第二个阶段， 提交/执行阶段</h4><p>根据各参与者反馈的结果， 协 调者会确定是提交还是回滚操作； </p><p>基于二阶段提交会存在一些问题： </p><ul><li><p>性能问题。 从流程上我们可以看得出， 其最大缺点就是在于穹的执行过程中间， 节点都处于阻塞状态。 各个操作数据库的节点都点用的数据库资源， 只有当所有节点准备完毕， 事务协调者才会通知进行全局提交， 参与者进行本地事务提交后才会释放资源。 这样的过程会比较漫长，对性参影响比较大。 </p></li><li><p>协调者单点故障问题， 事务协调者是整个XA模型的核心， 一旦事务协调者挂掉， 会导致参与者收不到提交或回滚的通知， 从而导致参与者节点绐xtlu处理事务无法完成的中间状态。 </p></li><li><p>丢失消息导致的数据不一致问题， 在这二个阶段， 如果发生局部网络问题， 一部分事务参考者收一了提交消息， 别一部分事务参与者没收到提交消息， 那么就会导致节点间数据的不一致问题。 </p></li></ul><h2 id="三阶段提交（3PC）"><a href="#三阶段提交（3PC）" class="headerlink" title="三阶段提交（3PC）"></a>三阶段提交（3PC）</h2><h2 id="补偿事务（TCC）"><a href="#补偿事务（TCC）" class="headerlink" title="补偿事务（TCC）"></a>补偿事务（TCC）</h2><p>TCC（Try-Confirm-Cancel） 又称补偿事务。 其核心思想是： ”针对每一个操作都要注册一个与其对应的确认和补偿（撤销操作）“ 它分为三个操作；</p><ul><li><p>Try阶段： 主要是对业务系统做检测与资源预留</p></li><li><p>Confirm阶段： 确认执行业务操作</p></li><li><p>Cancel阶段： 取消执行业务操作； </p></li></ul><h2 id="Redis实际，使用场景"><a href="#Redis实际，使用场景" class="headerlink" title="Redis实际，使用场景"></a>Redis实际，使用场景</h2><ul><li><p>Cache Aside </p></li><li><p>Read/Write Through</p></li></ul><p>对应的流程图为：<br><img src="https://www.ant-loiter.com/img/redis-cache-strage.png" alt="Redis Cache strategy"></p><p>实战：</p><ul><li><p>数据加版本号， 写库时自动增一。 更新缓存时， 只允许高版本覆盖低版本的数据。 </p></li><li><p>对于Cache Aside 和 Read/Write Through而带来的数据不一致问题， 工作中是这样解决：</p><ul><li>a 写线程； b 读线程； </li><li>b 线程， 读缓存 -&gt; 未命中 -&gt; 上写锁 -&gt; 从db读数据到缓存 -&gt; 释放锁； </li><li>a 线程， 上写锁 -&gt; 写DB -&gt; 删除缓存/ 改缓存 -&gt; 释放锁；<br>这样来保证a，b线程并发读写缓存带来的脏数据问题。 </li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;分布式事务这深入理解？&quot;&gt;&lt;a href=&quot;#分布式事务这深入理解？&quot; class=&quot;headerlink&quot; title=&quot;分布式事务这深入理解？&quot;&gt;&lt;/a&gt;分布式事务这深入理解？&lt;/h1&gt;&lt;p&gt;&lt;em&gt;2PC、3PC及TCC协议&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=
      
    
    </summary>
    
      <category term="Java Enterprise Architest Info" scheme="https://www.ant-loiter.com/categories/Java-Enterprise-Architest-Info/"/>
    
    
      <category term="Java Enterprise Architest Info" scheme="https://www.ant-loiter.com/tags/Java-Enterprise-Architest-Info/"/>
    
  </entry>
  
  <entry>
    <title>JavaEnterprise_Architect</title>
    <link href="https://www.ant-loiter.com/2020/06/12/JavaEnterprise-Architect/"/>
    <id>https://www.ant-loiter.com/2020/06/12/JavaEnterprise-Architect/</id>
    <published>2020-06-12T06:33:38.000Z</published>
    <updated>2020-10-30T13:49:33.118Z</updated>
    
    <content type="html"><![CDATA[<h2 id="不同场景数据存储分析"><a href="#不同场景数据存储分析" class="headerlink" title="不同场景数据存储分析"></a>不同场景数据存储分析</h2><h3 id="购物车"><a href="#购物车" class="headerlink" title="购物车"></a>购物车</h3><p>  购物车的数据，分二个场景：<br>    场景一： 匿名购物车<br>    场景二： 实名购物车<br>  场景一的情况， 建议是要把数据存在客户端。 那客户端可以存储数据的空间可以是以下三个：<br>    1、 Cookie<br>    2、 Session<br>    3、 LocalStorage<br>  其中Session是不太合适的， 原因是Session的保留时间短， 而且Session的数据实际上还是保存在服务端的。<br>  Cookie 和 LocalStorage 最关键的区别是， 客户端和服务端的每次交互， 都会自动带着Cookie数据的。 这样服务端可以读写客户端Cookie中的数据， 而LocalStorage里的数据， 只能由客户端来访问。 所以使用Cookie来存储， 实现起来比较简单， 加减合并购物车的过程中， 由于服务端可以读写Cookie， 这样全部逻辑都可以在服务端实现。 并且客户端和服务端请求在次数也相对少一些。 但是Cookie在客户端空间只有4K， 有时可能空间不够用。 所有有时选择LocalStorage反而又更合适。 </p><p>  一般购物车保存的数据格式都是一样的， 如下JSON表示：<br>  <figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">// 购物车数据格式</span><br><span class="line">  &#123;</span><br><span class="line">      <span class="attr">"cart"</span>: [</span><br><span class="line">          &#123;</span><br><span class="line">              <span class="attr">"SKUID"</span>: <span class="number">8888</span>,</span><br><span class="line">              <span class="attr">"timestamp"</span>: <span class="number">1578721136</span>,</span><br><span class="line">              <span class="attr">"count"</span>: <span class="number">1</span>,</span><br><span class="line">              <span class="attr">"selected"</span>: <span class="literal">true</span></span><br><span class="line">          &#125;,</span><br><span class="line">          &#123;</span><br><span class="line">              <span class="attr">"SKUID"</span>: <span class="number">6666</span>,</span><br><span class="line">              <span class="attr">"timestamp"</span>: <span class="number">1578721138</span>,</span><br><span class="line">              <span class="attr">"count"</span>: <span class="number">2</span>,</span><br><span class="line">              <span class="attr">"selected"</span>: <span class="literal">false</span></span><br><span class="line">          &#125;</span><br><span class="line">      ]</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></p><p>  用户登录态的购物车存储，应该怎么处理呢？ </p><p>  实名态的用户购物车，就必须要存储在服务端了。 常规的思路是， 设计一张购物车的表， 把数据存在MySQL中， 这个表的结构周样可以参照刚讲的实体模型来设计 ： </p><p>  也可以用Redis来保存购物车数据， 以用户ID作为Key， 用一个Redis的HASH作来Value来保存购物车中的商品， 比如；<br>  <figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">// Redis Hash Info</span><br><span class="line">  &#123;</span><br><span class="line">      <span class="attr">"KEY"</span>: <span class="number">6666</span>,</span><br><span class="line">      <span class="attr">"VALUE"</span>: [</span><br><span class="line">          &#123;</span><br><span class="line">              <span class="attr">"FIELD"</span>: <span class="number">8888</span>,</span><br><span class="line">              <span class="attr">"FIELD_VALUE"</span>: &#123;</span><br><span class="line">                  <span class="attr">"timestamp"</span>: <span class="number">1578721136</span>,</span><br><span class="line">                  <span class="attr">"count"</span>: <span class="number">1</span>,</span><br><span class="line">                  <span class="attr">"selected"</span>: <span class="literal">true</span></span><br><span class="line">              &#125;</span><br><span class="line">          &#125;,</span><br><span class="line">          &#123;</span><br><span class="line">              <span class="attr">"FIELD"</span>: <span class="number">6666</span>,</span><br><span class="line">              <span class="attr">"FIELD_VALUE"</span>: &#123;</span><br><span class="line">                  <span class="attr">"timestamp"</span>: <span class="number">1578721138</span>,</span><br><span class="line">                  <span class="attr">"count"</span>: <span class="number">2</span>,</span><br><span class="line">                  <span class="attr">"selected"</span>: <span class="literal">false</span></span><br><span class="line">              &#125;</span><br><span class="line">          &#125;</span><br><span class="line">      ]</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></p><p>  存储的地儿， 一般会有二种选择： 1、 Redis ; 2、 MySQL ; </p><p>  原则： <strong>读多写少用缓存Redis; 写多读少用MQ</strong></p><h2 id="使用数据库事务来保证数据一致性"><a href="#使用数据库事务来保证数据一致性" class="headerlink" title="使用数据库事务来保证数据一致性"></a>使用数据库事务来保证数据一致性</h2><p>  一般在设计对外提供的服务接口时， 不能提供单独更新余额或者流水功能， 只提供交易功能。 我们需要在实现交易功能的时候， 同时记录流水并修改余额， 并且要尽可能保证， 在任何情况下， 记录流水和修改余额这两个操作， 要么都成功， 要么都失败。 不能有任何一笔交易出现。 记录了流水但余额没更新， 或者更新了余额保是没有记录流水。 </p><p>  事物特性： 原子性Atomic; 一致性Consistency; 隔离性Lsolation; 持久性Durability; ACID</p><p>  对账户系统和其他大多数交易系统来说， 事务的原子性和持久性是必须要保证的， 否则就失去了使用事务的意义， 而一致性和隔离性其实可以做适当牺牲， 不换取性能。 所以MySQL提供了四种事物隔离级别， </p><table><thead><tr><th style="text-align:center">隔离级别</th><th style="text-align:center">脏读</th><th style="text-align:center">不可重复读</th><th style="text-align:center">幻读</th></tr></thead><tbody><tr><td style="text-align:center">读未提交</td><td style="text-align:center">Y</td><td style="text-align:center">Y</td><td style="text-align:center">Y</td></tr><tr><td style="text-align:center">读已提交</td><td style="text-align:center">N</td><td style="text-align:center">Y</td><td style="text-align:center">Y</td></tr><tr><td style="text-align:center">可重复读</td><td style="text-align:center">N</td><td style="text-align:center">N</td><td style="text-align:center">Y</td></tr><tr><td style="text-align:center">串行执行</td><td style="text-align:center">N</td><td style="text-align:center">N</td><td style="text-align:center">N</td></tr></tbody></table><p>   主要的隔离级别有二个， 一个是RC 和 RR 这二种；<br>   RC： 表示读已提交； 这个隔离级别是可能会存在重复读的情况；<br>   RR： 表示不可重复读， 这个隔离级别是会隔离重复读的情况；<br>   MySQL是默认事物隔离级别是 RR； Oracle默认的事物隔离级别为 RC</p><p> 幻读， 就是在操作时感觉是出现了幻觉一样。 场景， 数据库正在处理一个RC或RR的事物操作场景中， 现在有一个线程需要往数据库中插入一条记录； 在插入之前查询是没有与要插入ID冲突的记录， 但是在真正插入时，便 报ID冲突。 这种情况就是幻读现像； </p><p> 方案： 兼顾并发、性能和数据一致性的交易实现， 这个实现在隔离级别为RC和RR时， 都是安全的。 </p><p> 1、 我们给账户余额表增加一个log_id属性， 记录最后一笔交易的流水号；<br> 2、 首先开启事物， 查询并记录当前账户的余额和最后和笔交易的流水号；<br> 3、然后写入流水记录。<br> 4、再更新账户余额， 需要在更新语句中where条件中限定， 只有流水号等于之前查询出来的流水号时才更新。<br> 5、 然后检查更新的返回值， 如果更新成功就提交事物， 否则回滚事务；<br> <img src="https://www.ant-loiter.com/img/RCRR.png" alt="时序图表示"></p><p> 需要注意的时， 确认更新成功不是以catch为准， 而是要确认成功更新的记录数。 以下是交易SQL：<br> <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"> mysql&gt; begin;</span><br><span class="line"> Query OK, 0 rows affected (0.00 sec)</span><br><span class="line"> mysql&gt;  -- 查询当前账户的余额和最后一笔交易的流水号。</span><br><span class="line"> mysql&gt; select balance, log_id from account_balance where user_id = 0;+---------+--------+</span><br><span class="line"> | balance | log_id |</span><br><span class="line"> +<span class="comment">---------+--------+</span></span><br><span class="line"> |     100 |      3 |</span><br><span class="line"> +<span class="comment">---------+--------+</span></span><br><span class="line"> 1 row in <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span><br><span class="line"> mysql&gt;  <span class="comment">-- 插入流水记录。</span></span><br><span class="line"> mysql&gt; <span class="keyword">insert</span> <span class="keyword">into</span> account_log <span class="keyword">values</span>    </span><br><span class="line">      -&gt; (<span class="literal">NULL</span>, <span class="number">100</span>, <span class="keyword">NOW</span>(), <span class="number">1</span>, <span class="number">1001</span>, <span class="literal">NULL</span>, <span class="number">0</span>, <span class="literal">NULL</span>, <span class="number">0</span>, <span class="number">0</span>);</span><br><span class="line">      Query OK, 1 row affected (0.01 sec)</span><br><span class="line">mysql&gt;  -- 更新余额，注意where条件中，限定了只有流水号等于之前查询出的流水号3时才更新。</span><br><span class="line">mysql&gt; update account_balance    </span><br><span class="line">-&gt; set balance = balance + 100, log_id = LAST_INSERT_ID(), timestamp = NOW()    </span><br><span class="line">-&gt; where user_id = 0 and log_id = 3;</span><br><span class="line">Query OK, 1 row affected (0.00 sec)</span><br><span class="line">Rows matched: 1  Changed: 1  Warnings: 0</span><br><span class="line">mysql&gt;  -- 这里需要检查更新结果，只有更新余额成功（Changed: 1）才提交事务，否则回滚事务。</span><br><span class="line">mysql&gt; commit;</span><br><span class="line">Query OK, 0 rows affected (0.01 sec)</span><br></pre></td></tr></table></figure></p><p>流水和余额两个表的DDL，如下：<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`account_log`</span> (</span><br><span class="line">  <span class="string">`log_id`</span> <span class="built_in">int</span> <span class="keyword">NOT</span> <span class="literal">NULL</span> AUTO_INCREMENT <span class="keyword">COMMENT</span> <span class="string">'流水号'</span>,</span><br><span class="line">  <span class="string">`amount`</span> <span class="built_in">int</span> <span class="keyword">NOT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'交易金额'</span>,</span><br><span class="line">  <span class="string">`timestamp`</span> datetime <span class="keyword">NOT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'时间戳'</span>,</span><br><span class="line">  <span class="string">`from_system`</span> <span class="built_in">int</span> <span class="keyword">NOT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'转出系统编码'</span>,</span><br><span class="line">  <span class="string">`from_system_transaction_number`</span> <span class="built_in">int</span> <span class="keyword">DEFAULT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'转出系统的交易号'</span>,</span><br><span class="line">  <span class="string">`from_account`</span> <span class="built_in">int</span> <span class="keyword">DEFAULT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'转出账户'</span>,</span><br><span class="line">  <span class="string">`to_system`</span> <span class="built_in">int</span> <span class="keyword">NOT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'转入系统编码'</span>,</span><br><span class="line">  <span class="string">`to_system_transaction_number`</span> <span class="built_in">int</span> <span class="keyword">DEFAULT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'转入系统的交易号'</span>,</span><br><span class="line">  <span class="string">`to_account`</span> <span class="built_in">int</span> <span class="keyword">DEFAULT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'转入账户'</span>,</span><br><span class="line">  <span class="string">`transaction_type`</span> <span class="built_in">int</span> <span class="keyword">NOT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'交易类型编码'</span>,</span><br><span class="line">  PRIMARY <span class="keyword">KEY</span> (<span class="string">`log_id`</span>)</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`account_balance`</span> (</span><br><span class="line">  <span class="string">`user_id`</span> <span class="built_in">int</span> <span class="keyword">NOT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'用户ID'</span>,</span><br><span class="line">  <span class="string">`balance`</span> <span class="built_in">int</span> <span class="keyword">NOT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'余额'</span>,</span><br><span class="line">  <span class="string">`timestamp`</span> datetime <span class="keyword">NOT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'时间戳'</span>,</span><br><span class="line">  <span class="string">`log_id`</span> <span class="built_in">int</span> <span class="keyword">NOT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'最后一笔交易的流水号'</span>,</span><br><span class="line">  PRIMARY <span class="keyword">KEY</span> (<span class="string">`user_id`</span>)</span><br><span class="line">);</span><br></pre></td></tr></table></figure></p><h3 id="结论："><a href="#结论：" class="headerlink" title="结论："></a>结论：</h3><p>账户系统用于记录每个用户的余额， 为了保证数据的可追溯性，还需要记录账户流水。 流水记录只能新增， 任何情况下都不允许修改和删除， 每次交易的时候需要把流水和余额放在同一个事务中一起更新； </p><p>事务具备原子性（A）， 一致性（C）， 隔离性（I）， 持久性（D）四种基本特性， 也就是ACID， 它可以保证在一个事务中执行的数据更新， 要么都成功， 要么都失败。 并且在事务执行过程中， 中间状态的数据对其他事务是不可见的。 </p><p>ACID是一种理想情况， 特别是要完美地实现CI，会导致数据库性能严重下降， 所以MySQL提供的四种可选的隔离级别， 牺牲一定的隔离性和一致性， 用于换取高性能。 这四种隔离级别中， 只有RC 和 RR 这两个隔离级别是常用的。 它们中唯一区别是在进行的事务中， 其他事务对数据的更新是否可见。 </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;不同场景数据存储分析&quot;&gt;&lt;a href=&quot;#不同场景数据存储分析&quot; class=&quot;headerlink&quot; title=&quot;不同场景数据存储分析&quot;&gt;&lt;/a&gt;不同场景数据存储分析&lt;/h2&gt;&lt;h3 id=&quot;购物车&quot;&gt;&lt;a href=&quot;#购物车&quot; class=&quot;header
      
    
    </summary>
    
      <category term="Java Enterprise Architest Info" scheme="https://www.ant-loiter.com/categories/Java-Enterprise-Architest-Info/"/>
    
    
      <category term="Java Enterprise Architest Info" scheme="https://www.ant-loiter.com/tags/Java-Enterprise-Architest-Info/"/>
    
  </entry>
  
  <entry>
    <title>mybatis-plus-notepad</title>
    <link href="https://www.ant-loiter.com/2020/05/23/mybatis-plus-notepad/"/>
    <id>https://www.ant-loiter.com/2020/05/23/mybatis-plus-notepad/</id>
    <published>2020-05-23T04:38:00.000Z</published>
    <updated>2020-10-30T13:49:33.120Z</updated>
    
    <content type="html"><![CDATA[<h1 id="MyBatis-Plus（简称-MP）是一个-MyBatis-的增强工具，在-MyBatis-的基础上只做增强不做改变，为简化开发、提高效率而生。"><a href="#MyBatis-Plus（简称-MP）是一个-MyBatis-的增强工具，在-MyBatis-的基础上只做增强不做改变，为简化开发、提高效率而生。" class="headerlink" title="MyBatis-Plus（简称 MP）是一个 MyBatis 的增强工具，在 MyBatis 的基础上只做增强不做改变，为简化开发、提高效率而生。"></a>MyBatis-Plus（简称 MP）是一个 MyBatis 的增强工具，在 MyBatis 的基础上只做增强不做改变，为简化开发、提高效率而生。</h1><h1 id="特性"><a href="#特性" class="headerlink" title="特性"></a>特性</h1><ul><li>无侵入：只做增强不做改变，引入它不会对现有工程产生影响，如丝般顺滑</li><li>损耗小：启动即会自动注入基本 CURD，性能基本无损耗，直接面向对象操作</li><li>强大的 CRUD 操作：内置通用 Mapper、通用 Service，仅仅通过少量配置即可实现单表大部分 CRUD 操作，更有强大的条件构造器，满足各类使用需求</li><li>支持 Lambda 形式调用：通过 Lambda 表达式，方便的编写各类查询条件，无需再担心字段写错</li><li>支持主键自动生成：支持多达 4 种主键策略（内含分布式唯一 ID 生成器 - Sequence），可自由配置，完美解决主键问题</li><li>支持 ActiveRecord 模式：支持 ActiveRecord 形式调用，实体类只需继承 Model 类即可进行强大的 CRUD 操作</li><li>支持自定义全局通用操作：支持全局通用方法注入（ Write once, use anywhere ）</li><li>内置代码生成器：采用代码或者 Maven 插件可快速生成 Mapper 、 Model 、 Service 、 Controller 层代码，支持模板引擎，更有超多自定义配置等您来使用</li><li>内置分页插件：基于 MyBatis 物理分页，开发者无需关心具体操作，配置好插件之后，写分页等同于普通 List 查询</li><li>分页插件支持多种数据库：支持 MySQL、MariaDB、Oracle、DB2、H2、HSQL、SQLite、Postgre、SQLServer 等多种数据库</li><li>内置性能分析插件：可输出 Sql 语句以及其执行时间，建议开发测试时启用该功能，能快速揪出慢查询</li><li>内置全局拦截插件：提供全表 delete 、 update 操作智能分析阻断，也可自定义拦截规则，预防误操作</li></ul><h1 id="支持数据库"><a href="#支持数据库" class="headerlink" title="支持数据库"></a>支持数据库</h1><ul><li><p>mysql 、 mariadb 、 oracle 、 db2 、 h2 、 hsql 、 sqlite 、 postgresql 、 sqlserver</p></li><li><p>达梦数据库 、 虚谷数据库 、 人大金仓数据库</p></li></ul><h1 id="框架结构"><a href="#框架结构" class="headerlink" title="框架结构"></a>框架结构</h1><p><a href="https://mybatis.plus/img/mybatis-plus-framework.jpg" target="_blank" rel="noopener">架构图</a></p><h1 id="mp分页"><a href="#mp分页" class="headerlink" title="mp分页"></a>mp分页</h1><ul><li><p>在spring boot项目中注册分页插件</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//Spring boot方式</span></span><br><span class="line"><span class="meta">@EnableTransactionManagement</span></span><br><span class="line"><span class="meta">@Configuration</span></span><br><span class="line"><span class="meta">@MapperScan</span>(<span class="string">"com.baomidou.cloud.service.*.mapper*"</span>)</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MybatisPlusConfig</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Bean</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> PaginationInterceptor <span class="title">paginationInterceptor</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        PaginationInterceptor paginationInterceptor = <span class="keyword">new</span> PaginationInterceptor();</span><br><span class="line">        <span class="comment">// 设置请求的页面大于最大页后操作， true调回到首页，false 继续请求  默认false</span></span><br><span class="line">        <span class="comment">// paginationInterceptor.setOverflow(false);</span></span><br><span class="line">        <span class="comment">// 设置最大单页限制数量，默认 500 条，-1 不受限制</span></span><br><span class="line">        <span class="comment">// paginationInterceptor.setLimit(500);</span></span><br><span class="line">        <span class="comment">// 开启 count 的 join 优化,只针对部分 left join</span></span><br><span class="line">        paginationInterceptor.setCountSqlParser(<span class="keyword">new</span> JsqlParserCountOptimize(<span class="keyword">true</span>));</span><br><span class="line">        <span class="keyword">return</span> paginationInterceptor;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>接下来就可以使用mapper里的page相关方法如：</p><ul><li>selectPage</li><li>selectMapspage</li><li>就是返回的泛型结构不太相同</li></ul></li><li>同时也可以自定义支持page的查询方法； </li></ul><h1 id="逻辑删除"><a href="#逻辑删除" class="headerlink" title="逻辑删除"></a>逻辑删除</h1><ul><li>可以在yml里全局配置逻辑删除标识</li><li>在实例上通过注解指定标记逻辑删除的字段</li><li>在mapper自带的方法里就可以进行逻辑删除了， 正常做删除就是逻辑删除</li><li>配置逻辑删除后， 自带的方法查询都会自动过滤逻辑删除的记录； 但是，但是，但是自定义的查询方法是不会自动过滤逻辑删除记录，需要自行处理； 处理方法有：<ul><li>在wrapper里添加条件，建议采用lambda Wrapper； </li><li>可以在mapper的方法的sql里添加条件； </li></ul></li></ul><h1 id="自动填充功能；"><a href="#自动填充功能；" class="headerlink" title="自动填充功能；"></a>自动填充功能；</h1><ul><li><p>需要confiation自动填充插件，如下： </p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Slf</span>4j</span><br><span class="line"><span class="meta">@Component</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyMetaObjectHandler</span> <span class="keyword">implements</span> <span class="title">MetaObjectHandler</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">insertFill</span><span class="params">(MetaObject metaObject)</span> </span>&#123;</span><br><span class="line">        log.info(<span class="string">"start insert fill ...."</span>);</span><br><span class="line">        <span class="keyword">this</span>.strictInsertFill(metaObject, <span class="string">"createTime"</span>, LocalDateTime.class, LocalDateTime.now()); <span class="comment">// 起始版本 3.3.0(推荐使用)</span></span><br><span class="line">        <span class="keyword">this</span>.fillStrategy(metaObject, <span class="string">"createTime"</span>, LocalDateTime.now()); <span class="comment">// 也可以使用(3.3.0 该方法有bug请升级到之后的版本如`3.3.1.8-SNAPSHOT`)</span></span><br><span class="line">        <span class="comment">/* 上面选其一使用,下面的已过时(注意 strictInsertFill 有多个方法,详细查看源码) */</span></span><br><span class="line">        <span class="comment">//this.setFieldValByName("operator", "Jerry", metaObject);</span></span><br><span class="line">        <span class="comment">//this.setInsertFieldValByName("operator", "Jerry", metaObject);</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">updateFill</span><span class="params">(MetaObject metaObject)</span> </span>&#123;</span><br><span class="line">        log.info(<span class="string">"start update fill ...."</span>);</span><br><span class="line">        <span class="keyword">this</span>.strictUpdateFill(metaObject, <span class="string">"updateTime"</span>, LocalDateTime.class, LocalDateTime.now()); <span class="comment">// 起始版本 3.3.0(推荐使用)</span></span><br><span class="line">        <span class="keyword">this</span>.fillStrategy(metaObject, <span class="string">"updateTime"</span>, LocalDateTime.now()); <span class="comment">// 也可以使用(3.3.0 该方法有bug请升级到之后的版本如`3.3.1.8-SNAPSHOT`)</span></span><br><span class="line">        <span class="comment">/* 上面选其一使用,下面的已过时(注意 strictUpdateFill 有多个方法,详细查看源码) */</span></span><br><span class="line">        <span class="comment">//this.setFieldValByName("operator", "Tom", metaObject);</span></span><br><span class="line">        <span class="comment">//this.setUpdateFieldValByName("operator", "Tom", metaObject);</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>需要在实例是属性上通过注解指定自动填充策略，如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 注意！这里需要标记为填充字段</span></span><br><span class="line"><span class="meta">@TableField</span>(.. fill = FieldFill.INSERT)</span><br><span class="line"><span class="keyword">private</span> String fillField;</span><br></pre></td></tr></table></figure></li><li><p>但是实际中自动填充还支持递进关系， 可以支持临时设置的值覆盖原有的逻辑，但需要在MyMetaObjectHandler类里进行逻辑配置； </p></li><li><p>还可以在MyMetaObjectHandler配置这个字段是否存在而动态的进行自动填充策略； </p></li></ul><h1 id="乐观锁"><a href="#乐观锁" class="headerlink" title="乐观锁"></a>乐观锁</h1><p><em>一般查询多写入少乐观锁；写入读少悲观锁</em></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;MyBatis-Plus（简称-MP）是一个-MyBatis-的增强工具，在-MyBatis-的基础上只做增强不做改变，为简化开发、提高效率而生。&quot;&gt;&lt;a href=&quot;#MyBatis-Plus（简称-MP）是一个-MyBatis-的增强工具，在-MyBatis-
      
    
    </summary>
    
      <category term="mybatis mybatis-plus orm" scheme="https://www.ant-loiter.com/categories/mybatis-mybatis-plus-orm/"/>
    
    
      <category term="mybatis mybatis-plus orm" scheme="https://www.ant-loiter.com/tags/mybatis-mybatis-plus-orm/"/>
    
  </entry>
  
  <entry>
    <title>SkyWalking</title>
    <link href="https://www.ant-loiter.com/2020/05/18/SkyWalking/"/>
    <id>https://www.ant-loiter.com/2020/05/18/SkyWalking/</id>
    <published>2020-05-18T02:53:51.000Z</published>
    <updated>2020-10-30T13:49:33.119Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://hacpai.com/article/1554982301423" target="_blank" rel="noopener">https://hacpai.com/article/1554982301423</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;a href=&quot;https://hacpai.com/article/1554982301423&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://hacpai.com/article/1554982301423&lt;/a&gt;&lt;/p&gt;

      
    
    </summary>
    
    
      <category term="SkyWalking linkln Lookup" scheme="https://www.ant-loiter.com/tags/SkyWalking-linkln-Lookup/"/>
    
  </entry>
  
  <entry>
    <title>Redis-deploy</title>
    <link href="https://www.ant-loiter.com/2020/05/12/Redis-deploy/"/>
    <id>https://www.ant-loiter.com/2020/05/12/Redis-deploy/</id>
    <published>2020-05-12T06:20:11.000Z</published>
    <updated>2020-10-30T13:49:33.119Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Deploy-standalone-Redis-Server"><a href="#Deploy-standalone-Redis-Server" class="headerlink" title="Deploy standalone Redis Server"></a>Deploy standalone Redis Server</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ wget http://download.redis.io/releases/redis-6.0.1.tar.gz</span><br><span class="line">$ tar xzf redis-6.0.1.tar.gz</span><br><span class="line">$ cd redis-6.0.1</span><br><span class="line">$ make</span><br></pre></td></tr></table></figure><p>以下是从官网拷贝，如果在centos7 64bit中安装会不成功， make不通过。 可能原因主是gcc版本不对，gcc版本要5.3 + ，centos7默认源的gcc版本为4.8.5. 升级gcc：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$  yum -y install gcc  // 先安装，如果已经安装略过</span><br><span class="line">$  gcc -v 查看当前版本</span><br><span class="line">$  yum -y install centos-release-scl</span><br><span class="line">$  yum -y install devtoolset-9-gcc devtoolset-9-gcc-c++ deltoolset-9-binutils</span><br><span class="line">$  scl enable devtoolset-9 bash   </span><br><span class="line">$  gcc -v  完成升有，此时升级到了9.x.x版本</span><br><span class="line">``` </span><br><span class="line"></span><br><span class="line">*以上操作scl命令启用只是临时生效，退出shell或重新打开一个shell就会恢复原系统gcc版本*</span><br></pre></td></tr></table></figure></p><p> echo “source /opt/rh/devtoolset-9/enable” &gt;&gt; /etc/profile<br> 执行完后， 即可每次都会生效成gcc 9.X版本； </p><p> 接下来就可以make redis了，<br> 小技巧： make -jn (n建议是&lt;= cpu cos 数量)<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">启动：</span><br></pre></td></tr></table></figure></p><p>// 修改配置<br>vim ./redis.conf<br>    daemonize  on   支持后台运行<br>    bind 0.0.0.0<br>    logfile  “xxxxx.log”<br>    dbfilename “dump-xxx.rdb”<br>    rdbcompression yes</p><p>redis-server ./redis.conf  (这里指定的配置文件需要根据实际情况指定)</p><p>lsof -i:6379  检查是否启动成功</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">到些完成了， standalone 模式的启动； </span><br><span class="line"></span><br><span class="line">## Deploy Redis master/slave 模式</span><br><span class="line"></span><br><span class="line">*一个master节点二个slave节点*</span><br></pre></td></tr></table></figure><p> config master<br>      bind 0.0.0.0<br>      port 6379<br>      logfile “…”<br>      dbfilename “…”<br>      daemonize on<br>      rdbcompression yes<br> config slave 1<br>      bind 0.0.0.0<br>      port 6380<br>      logfile “…”<br>      dbfilename “…”<br>      daemonize on<br>      rdbcompression yes<br>      slaveof master 6379<br> config alave 2<br>      bind 0.0.0.0<br>      port 6381<br>      logfile “…”<br>      dbfilename “…”<br>      daemonize on<br>      rdbcompression yes<br>      slaveof master 6379<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">配置完成后， 就可以分别启动。</span><br></pre></td></tr></table></figure></p><p>  redis-server master.conf<br>  redis-server slave1.conf<br>  redis-server slave2.conf<br><code>`</code></p><h4 id="master-主数据持久化机制"><a href="#master-主数据持久化机制" class="headerlink" title="master 主数据持久化机制"></a>master 主数据持久化机制</h4><ul><li><p>RDB</p><p>save 900 1<br>save 300  10<br>save 60    1000</p></li><li><p>AOF</p><p>appendfsync  always<br>appendfsync  everysec<br>appendfsync  no</p></li></ul><h2 id="Deploy-Redis-Sentinel"><a href="#Deploy-Redis-Sentinel" class="headerlink" title="Deploy Redis Sentinel"></a>Deploy Redis Sentinel</h2><ul><li>Redis Sentinel 规划</li></ul><table><thead><tr><th style="text-align:center">IP</th><th style="text-align:center">端口号</th><th style="text-align:center">角色</th></tr></thead><tbody><tr><td style="text-align:center">201</td><td style="text-align:center">7000</td><td style="text-align:center">Redis Master</td></tr><tr><td style="text-align:center">202</td><td style="text-align:center">7000</td><td style="text-align:center">Redis Master</td></tr><tr><td style="text-align:center">203</td><td style="text-align:center">7000</td><td style="text-align:center">Redis Master</td></tr><tr><td style="text-align:center">201</td><td style="text-align:center">27000</td><td style="text-align:center">sentinel</td></tr><tr><td style="text-align:center">202</td><td style="text-align:center">27000</td><td style="text-align:center">sentinel</td></tr><tr><td style="text-align:center">203</td><td style="text-align:center">27000</td><td style="text-align:center">sentinel</td></tr></tbody></table>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Deploy-standalone-Redis-Server&quot;&gt;&lt;a href=&quot;#Deploy-standalone-Redis-Server&quot; class=&quot;headerlink&quot; title=&quot;Deploy standalone Redis Server&quot;&gt;
      
    
    </summary>
    
      <category term="redis master/slave sentinel cluster" scheme="https://www.ant-loiter.com/categories/redis-master-slave-sentinel-cluster/"/>
    
    
      <category term="redis master/slave sentinel cluster" scheme="https://www.ant-loiter.com/tags/redis-master-slave-sentinel-cluster/"/>
    
  </entry>
  
  <entry>
    <title>RocketMQ_Binder</title>
    <link href="https://www.ant-loiter.com/2020/05/08/RocketMQ-Binder/"/>
    <id>https://www.ant-loiter.com/2020/05/08/RocketMQ-Binder/</id>
    <published>2020-05-08T15:15:50.000Z</published>
    <updated>2020-10-30T13:49:33.119Z</updated>
    
    <content type="html"><![CDATA[<p>SCS (Spring cloud stream)</p><h2 id="RocketMQ-Binder-的使用"><a href="#RocketMQ-Binder-的使用" class="headerlink" title="RocketMQ Binder 的使用"></a>RocketMQ Binder 的使用</h2><ul><li><p>添加依赖</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">&lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt;</span><br><span class="line">&lt;artifactId&gt;spring-cloud-starter-stream-rocketmq&lt;/artifactId&gt;</span><br><span class="line">&lt;version&gt;$&#123;latest.version&#125;&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure></li><li><p>版本对应关系：<br>|Spring Cloud Version|Spring Cloud Alibaba Version|<br>|:—–:|:—–:|<br>|Spring Cloud Greenwich|2.1.0.RELEASE|<br>|Spring Cloud Finchley|2.0.0.RELEASE|<br>|Spring Cloud Edgware|1.5.0.RELEASE|</p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;SCS (Spring cloud stream)&lt;/p&gt;
&lt;h2 id=&quot;RocketMQ-Binder-的使用&quot;&gt;&lt;a href=&quot;#RocketMQ-Binder-的使用&quot; class=&quot;headerlink&quot; title=&quot;RocketMQ Binder 的使用&quot;&gt;
      
    
    </summary>
    
      <category term="RocketMQ Alibaba MQ Message" scheme="https://www.ant-loiter.com/categories/RocketMQ-Alibaba-MQ-Message/"/>
    
    
      <category term="RocketMQ Alibaba MQ Message" scheme="https://www.ant-loiter.com/tags/RocketMQ-Alibaba-MQ-Message/"/>
    
  </entry>
  
  <entry>
    <title>volatile</title>
    <link href="https://www.ant-loiter.com/2020/05/08/volatile/"/>
    <id>https://www.ant-loiter.com/2020/05/08/volatile/</id>
    <published>2020-05-08T08:04:06.000Z</published>
    <updated>2020-10-30T13:49:33.120Z</updated>
    
    <content type="html"><![CDATA[<h1 id="综述"><a href="#综述" class="headerlink" title="综述"></a>综述</h1><p>volatile让变量每次在使用的时候， 都从主存中取，而不是从各个线程的“工作内存”。<br>volatile变量具有synchronized的<em>可见性</em>和<em>有序性</em>， 但是不具备<em>原子特性</em>， 所以volatile变量并不保存并发的正确性； </p><p>在Java内存模型中， 有main memory， 每一个线程也有自已的memory(例如寄存器)。为了性能， 一个线程会在自己的memory中保持要访问的变量副本。 这样就会出现同一个变量在某个瞬间，在一个线程的memory中的值可能与另外一个线程memory中的值，或者main memory中的值不一致的情况， 一个变量声明为volatile，就意味着这个变量是随时会被其他线程修改的， 因此不能将它cache在线程的memory中。 </p><p>假如多个线程同时执行i++， volatile只能保证他们操作的i是同一块内存， 但依然可能出现写入脏数据的情况。 </p><p>volatile是非阻塞的。 </p><h1 id="volatile关键字的作用"><a href="#volatile关键字的作用" class="headerlink" title="volatile关键字的作用"></a>volatile关键字的作用</h1><ul><li>保证可见性（参看下面的“volatile的实现原理”）</li><li>禁止指令重排</li></ul><p>这里来看看第二个作用： 禁止指令重排。</p><p>重排序是指编译器和处理器为了优化程序性能而对指令序列进行排序的一种手段， 现代CPU中指令的执行次序不一定按排序执行， 没有相关性的指令可以打乱次序执行， 以充分种用cpu的指令流水线， 提高执行速度。 但是重排序也需要遵守一定规划： </p><ul><li>重排序操作不会对存在数据依赖关系的操作进行重排序。如： a=1;b=a;这个指令序列，由于第二个操作依赖于第一个操作， 所以在编译时和下理器运行时这两个操作不会被重排序。 </li><li>重排序是为了优化性能， 但是不管怎么重排序， 单线程下程序的执行结果不能被改变。</li></ul><p>若用volatile修饰共享变量，在编译时， 会在指令序列中插入内存屏障来禁止特定类型的处理器重排序。 </p><p>内存屏障是在代码中使用一些特殊指令， 如ARM中的dmb、dsb和isb指令， x86中的sfence、ifence和mfence指令。CPU遇到这些特殊指令后， 要等待前面的指令执行完成才执行后面的指令。 这些指令的作用就好像一道屏障把前后指令隔离开了， 防止cpu把前后两段指令颠倒执行。 （什么时内存屏障）</p><h1 id="正确使用volatile变量的条件"><a href="#正确使用volatile变量的条件" class="headerlink" title="正确使用volatile变量的条件"></a>正确使用volatile变量的条件</h1><p>要使用volatile变量提供理想的线程安全， 必须同时满足下面两个条件：</p><ul><li>对变量的写操作不依赖于当前值。</li><li>该变量没有包含在具有其他变量的不变式中</li></ul><p>第一个条件的限制使 volatile 变量不能用作线程安全计数器。虽然增量操作（x++）看上去类似一个单独操作，实际上它是一个由读取－修改－写入操作序列组成的组合操作，必须以原子方式执行，而 volatile 不能提供必须的原子特性。实现正确的操作需要使 x 的值在操作期间保持不变，而 volatile 变量无法实现这点。（然而，如果将值调整为只从单个线程写入，那么可以忽略第一个条件。）</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;综述&quot;&gt;&lt;a href=&quot;#综述&quot; class=&quot;headerlink&quot; title=&quot;综述&quot;&gt;&lt;/a&gt;综述&lt;/h1&gt;&lt;p&gt;volatile让变量每次在使用的时候， 都从主存中取，而不是从各个线程的“工作内存”。&lt;br&gt;volatile变量具有synchroniz
      
    
    </summary>
    
      <category term="volatile java synchronized" scheme="https://www.ant-loiter.com/categories/volatile-java-synchronized/"/>
    
    
      <category term="volatile java synchronized" scheme="https://www.ant-loiter.com/tags/volatile-java-synchronized/"/>
    
  </entry>
  
  <entry>
    <title>md5shasum</title>
    <link href="https://www.ant-loiter.com/2020/05/04/md5shasum/"/>
    <id>https://www.ant-loiter.com/2020/05/04/md5shasum/</id>
    <published>2020-05-04T06:55:48.000Z</published>
    <updated>2020-10-30T13:49:33.120Z</updated>
    
    <content type="html"><![CDATA[<h1 id="MD5验证"><a href="#MD5验证" class="headerlink" title="MD5验证"></a>MD5验证</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># md5 WebStorm-2019.2.4.dmg</span><br><span class="line">输出结果：</span><br><span class="line">MD5 (WebStorm-2019.2.4.dmg) = eaf1fb249706216e2ea162d947ee07ca</span><br></pre></td></tr></table></figure><h1 id="OpenSSL的MD5验证"><a href="#OpenSSL的MD5验证" class="headerlink" title="OpenSSL的MD5验证"></a>OpenSSL的MD5验证</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ openssl md5 WebStorm-2019.2.4.dmg </span><br><span class="line">输出结果：</span><br><span class="line">MD5(WebStorm-2019.2.4.dmg)= eaf1fb249706216e2ea162d947ee07ca</span><br></pre></td></tr></table></figure><h1 id="SHA1验证"><a href="#SHA1验证" class="headerlink" title="SHA1验证"></a>SHA1验证</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ shasum -a 1 WebStorm-2019.2.4.dmg </span><br><span class="line">输出结果：</span><br><span class="line">0a1df0eaf6a4fffd5800a460fc3a34d38f640ce4  WebStorm-2019.2.4.dmg</span><br></pre></td></tr></table></figure><h1 id="OpenSSL的SHA1验证"><a href="#OpenSSL的SHA1验证" class="headerlink" title="OpenSSL的SHA1验证"></a>OpenSSL的SHA1验证</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ openssl sha1 WebStorm-2019.2.4.dmg </span><br><span class="line">输出结果：</span><br><span class="line">SHA1(WebStorm-2019.2.4.dmg)= 0a1df0eaf6a4fffd5800a460fc3a34d38f640ce4</span><br></pre></td></tr></table></figure><h1 id="SHA256验证"><a href="#SHA256验证" class="headerlink" title="SHA256验证"></a>SHA256验证</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ shasum -a 256 WebStorm-2019.2.4.dmg </span><br><span class="line">输出结果：</span><br><span class="line">d2cae6370f2272c1c625774fa2552f670ea0e43af78c6b025df90c6ec73e0816  WebStorm-2019.2.4.dmg</span><br></pre></td></tr></table></figure><h1 id="OpenSSL的SHA256验证"><a href="#OpenSSL的SHA256验证" class="headerlink" title="OpenSSL的SHA256验证"></a>OpenSSL的SHA256验证</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ openssl sha256 WebStorm-2019.2.4.dmg </span><br><span class="line">输出结果：</span><br><span class="line">SHA256(WebStorm-2019.2.4.dmg)= d2cae6370f2272c1c625774fa2552f670ea0e43af78c6b025df90c6ec73e0816</span><br></pre></td></tr></table></figure><h1 id="SHA512验证"><a href="#SHA512验证" class="headerlink" title="SHA512验证"></a>SHA512验证</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ shasum -a 512 WebStorm-2019.2.4.dmg </span><br><span class="line">输出结果：</span><br><span class="line">6c5b7d7bec920c7f231d1dad3b4c1c85f4b0fa40ecbc47f0af78813f5c0201887c50d664e5494497411e1180d00396c2a744510a06765b87662302c9d3743e94  WebStorm-2019.2.4.dmg</span><br></pre></td></tr></table></figure><h1 id="OpenSSL的SHA512验证"><a href="#OpenSSL的SHA512验证" class="headerlink" title="OpenSSL的SHA512验证"></a>OpenSSL的SHA512验证</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ openssl sha512 WebStorm-2019.2.4.dmg </span><br><span class="line">输出结果：</span><br><span class="line">SHA512(WebStorm-2019.2.4.dmg)= 6c5b7d7bec920c7f231d1dad3b4c1c85f4b0fa40ecbc47f0af78813f5c0201887c50d664e5494497411e1180d00396c2a744510a06765b87662302c9d3743e94</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;MD5验证&quot;&gt;&lt;a href=&quot;#MD5验证&quot; class=&quot;headerlink&quot; title=&quot;MD5验证&quot;&gt;&lt;/a&gt;MD5验证&lt;/h1&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;
      
    
    </summary>
    
      <category term="md5 sha checksum signature" scheme="https://www.ant-loiter.com/categories/md5-sha-checksum-signature/"/>
    
    
      <category term="md5 sha checksum signature" scheme="https://www.ant-loiter.com/tags/md5-sha-checksum-signature/"/>
    
  </entry>
  
  <entry>
    <title>Disruptor</title>
    <link href="https://www.ant-loiter.com/2020/01/05/Disruptor/"/>
    <id>https://www.ant-loiter.com/2020/01/05/Disruptor/</id>
    <published>2020-01-05T11:30:36.000Z</published>
    <updated>2020-10-30T13:49:33.118Z</updated>
    
    <content type="html"><![CDATA[<h2 id="在-Disruptor中，-实现Hello-World需要如下几步骤："><a href="#在-Disruptor中，-实现Hello-World需要如下几步骤：" class="headerlink" title="在 Disruptor中， 实现Hello World需要如下几步骤："></a>在 Disruptor中， 实现Hello World需要如下几步骤：</h2><ul><li>建立一个Event类； </li><li>建立一个工厂Event类， 用于创建Event类实例对象</li><li>需要有一个监听事件类， 用于处理数据（Event类）</li><li>需要进行没度代码编写， 实例化Disruptor实例， 配置一系列参数， 然后我们对Disruptor实例绑定监听事件类， 接受并处理数据。 </li><li>在Disruptor中， 真正存储数据的核心叫做RingBuffer， 通过Disruptor实例拿到它， 然后把数据生产出来， 把数据加入到RingBuffer的实例对象中，即可。 </li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;在-Disruptor中，-实现Hello-World需要如下几步骤：&quot;&gt;&lt;a href=&quot;#在-Disruptor中，-实现Hello-World需要如下几步骤：&quot; class=&quot;headerlink&quot; title=&quot;在 Disruptor中， 实现Hello 
      
    
    </summary>
    
      <category term="disruptor 高并发 多线程 无锁" scheme="https://www.ant-loiter.com/categories/disruptor-%E9%AB%98%E5%B9%B6%E5%8F%91-%E5%A4%9A%E7%BA%BF%E7%A8%8B-%E6%97%A0%E9%94%81/"/>
    
    
      <category term="disruptor 高并发 多线程 无锁" scheme="https://www.ant-loiter.com/tags/disruptor-%E9%AB%98%E5%B9%B6%E5%8F%91-%E5%A4%9A%E7%BA%BF%E7%A8%8B-%E6%97%A0%E9%94%81/"/>
    
  </entry>
  
  <entry>
    <title>Hive_Data_Skew</title>
    <link href="https://www.ant-loiter.com/2019/07/21/Hive-Data-Skew/"/>
    <id>https://www.ant-loiter.com/2019/07/21/Hive-Data-Skew/</id>
    <published>2019-07-21T09:24:29.000Z</published>
    <updated>2020-10-30T13:49:33.118Z</updated>
    
    <content type="html"><![CDATA[<h2 id="倾斜原因：-map端缓慢，-输入数据文件多，-大小不均匀"><a href="#倾斜原因：-map端缓慢，-输入数据文件多，-大小不均匀" class="headerlink" title="倾斜原因： map端缓慢， 输入数据文件多， 大小不均匀"></a>倾斜原因： map端缓慢， 输入数据文件多， 大小不均匀</h2><ul><li><p>当出现的文件过多， 需要合并小文件， 可以通过</p><blockquote><p>set hive.merge.mapfiles=true 来解决， 将小文件进行合并；<br>set hive.map.aggr=true map端部分聚合， 相当于combiner 可以减少压力（默认 true)<br>set hive.groupby.skewindata=true 默认false</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">有数据倾斜的时候进行负载均衡，当选项设定为 true，生成的查询计划会有两个 MR Job。第一个 MR Job 中，Map 的输出结果集合会随机分布到 Reduce 中，每个 Reduce 做部分聚合操作，并输出结果，这样处理的结果是相同的 Group By Key 有可能被分发到不同的 Reduce 中，从而达到负载均衡的目的；第二个 MR Job 再根据预处理的数据结果按照 Group By Key 分布到 Reduce 中（这个过程可以保证相同的 Group By Key 被分布到同一个 Reduce 中），最后完成最终的聚合操作。</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>还可以通过配置map task 和reduce task进行调优 set mapred.map.tasks=number; set mapred.reduce.tasks=number;</p></blockquote></li><li><p>当映衬到一个大表和一个小表join时； </p><blockquote><p>小表在join的左侧， 大表在右侧，或使用mapjoin将小表加载到内存中。然后再与比较大的表进行join操作；如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select /* + MAPJOIN(a)*/ a.c1, b.c1,b.c2 from a join b where a.c1 = b.c1;</span><br></pre></td></tr></table></figure></blockquote></li><li><p>遇到需要进行join的但是关联字段有数据为null, 如表一的id需要和表二的id进行关联，null值的reduce会落在同一个节点上的问题； </p><blockquote><p>解决方法1： 子查询中过滤掉null值， id为空的不参于关联；<br>解决方法2： 用case when给空值分配随机的key值，（字符串+rand())，为了不让所有的null分配在同一个节点上； </p></blockquote></li><li><p>不同数据类型关联产生数据倾斜</p><blockquote><p>场景：一张表s8的日志，每个商品一条记录，要和商品表关联。但关联却碰到倾斜的问题。s8的日志中有字符串商品id,也有数字的商品id,类型是string的，但商品中的数字id是bigint的。猜测问题的原因是把s8的商品id转成数字id做hash来分配reduce，所以字符串id的s8日志，都到一个reduce上了，解决的方法验证了这个猜测。</p></blockquote><blockquote><p>解决方法： 把数字类型转换成字符串类型</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">select * from s8_log a </span><br><span class="line">Left outer join r_auction_auctions b </span><br><span class="line">On a.auction_id = cast(b.auction_id as string)</span><br></pre></td></tr></table></figure></blockquote></li><li><p>当Hive 的HQL中包含count(distinct)时</p><blockquote><p>如果数据量非常大， 执行 </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select a, count(distinct b) from t group by a;</span><br></pre></td></tr></table></figure></blockquote><p>时，会出现数据倾斜；</p><blockquote><p>解决方法： 使用 sum….group by 优化HQL，如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">select a, sum(1) from (</span><br><span class="line">  select a, b from t group by a, b</span><br><span class="line">) group by a;</span><br></pre></td></tr></table></figure></blockquote></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;倾斜原因：-map端缓慢，-输入数据文件多，-大小不均匀&quot;&gt;&lt;a href=&quot;#倾斜原因：-map端缓慢，-输入数据文件多，-大小不均匀&quot; class=&quot;headerlink&quot; title=&quot;倾斜原因： map端缓慢， 输入数据文件多， 大小不均匀&quot;&gt;&lt;/a&gt;倾斜
      
    
    </summary>
    
    
      <category term="hadoop hive dataSkew" scheme="https://www.ant-loiter.com/tags/hadoop-hive-dataSkew/"/>
    
  </entry>
  
  <entry>
    <title>BigData_Spark_001</title>
    <link href="https://www.ant-loiter.com/2019/06/13/BigData-Spark-001/"/>
    <id>https://www.ant-loiter.com/2019/06/13/BigData-Spark-001/</id>
    <published>2019-06-13T05:27:17.000Z</published>
    <updated>2020-10-30T13:49:33.118Z</updated>
    
    <content type="html"><![CDATA[<p>当下主流的计算方式有如下几种：</p><ul><li>批量处理， MapReduce、Hive、 Big</li><li>流式计算， storm</li><li>内存式的交互计算， presto， Impala</li></ul><h2 id="高效（比MapReduce快10-100倍）"><a href="#高效（比MapReduce快10-100倍）" class="headerlink" title="高效（比MapReduce快10-100倍）"></a>高效（比MapReduce快10-100倍）</h2><ul><li><p>内存计算引擎， 提供Cache机制来支持需要反复迭代计算或者多次数据共享， 减少数据读取IO开销 </p></li><li><p>DAG引擎， 减少多次计算之间中间结果写到HDFS的开销</p></li><li>使用多线程池模型来减少task启动开销， shuffle过程中避免不必要的sort操作以及减少磁盘IO操作（原先批量处理的shuffle过程主要完成的是sort + partition操作）</li></ul><h3 id="Spark核心概念—RDD"><a href="#Spark核心概念—RDD" class="headerlink" title="Spark核心概念—RDD"></a>Spark核心概念—RDD</h3><p>Resilient Distributed Datasets 弹性分布式数据集</p><ul><li>分布式存储在集群的各个节点上（每一部分称为一个Partition)</li><li>可以选择不同的存储方式（磁盘或内存）</li><li>可以由一个RDD生成另外一个RDD（转换操作）</li><li>数据丢失后可以自动恢复</li></ul><h3 id="RDD基本操作（operator）"><a href="#RDD基本操作（operator）" class="headerlink" title="RDD基本操作（operator）"></a>RDD基本操作（operator）</h3><ul><li><p>Transformation (转换)<br>通过已有的RDD产生新的RDD<br>举例， map, filter, groupBy, reduceBy</p></li><li><p>Action(动作)<br>通过RDD计算得到一个或者一组值 ；<br>举例： count、 reduce、 saveAsTextFile</p></li><li><p>初始RDD的生成<br>可通过scala集合或者Hadoop数据集构造； </p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;当下主流的计算方式有如下几种：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;批量处理， MapReduce、Hive、 Big&lt;/li&gt;
&lt;li&gt;流式计算， storm&lt;/li&gt;
&lt;li&gt;内存式的交互计算， presto， Impala&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;高效（比MapR
      
    
    </summary>
    
    
      <category term="BigData Spark MapReduce" scheme="https://www.ant-loiter.com/tags/BigData-Spark-MapReduce/"/>
    
  </entry>
  
  <entry>
    <title>Hive_FIle_format_eg</title>
    <link href="https://www.ant-loiter.com/2019/06/08/Hive-FIle-format-eg/"/>
    <id>https://www.ant-loiter.com/2019/06/08/Hive-FIle-format-eg/</id>
    <published>2019-06-08T07:36:58.000Z</published>
    <updated>2020-10-30T13:49:33.118Z</updated>
    
    <content type="html"><![CDATA[<p>概述</p><p>只要是配置了正确的文件类型和压缩类型(比如Textfile+Gzip、SequenceFile+Snappy等)，Hive都可以按预期读取并解析数据，提供SQL功能。</p><p>SequenceFile本身的结构已经设计了内容进行压缩。所以对于SequenceFile文件的压缩，并不是先生成SequenceFile文件，再对文件进行压缩。而是生成SequenceFile文件时，对其中的内容字段进行压缩。最终压缩后，对外仍体现为一个SequenceFile。</p><p>RCFile、ORCFile、Parquet、Avro对于压缩的处理方式与SequenceFile相同。</p><p>文件格式</p><p>Textfile<br>SequenceFile<br>RCFile<br>ORCFile<br>Parquet<br>Avro<br>压缩算法的编解码器</p><p>TEXTFILE</p><p>–创建一个表，格式为文本文件：<br>CREATE EXTERNAL TABLE student_text (id STRING, name STRING)<br>ROW FORMAT DELIMITED<br>    FIELDS TERMINATED BY ‘,’<br>    LINES TERMINATED BY ‘\n’<br>STORED AS TEXTFILE;<br>–导入数据到此表中,将启动MR任务<br>INSERT OVERWRITE TABLE student_text SELECT * FROM student;<br>可查看到生成的数据文件的格式为非压缩的文本文件：</p><p>hdfs dfs -cat /user/hive/warehouse/student_text/000000_0 </p><p>1001810081,cheyo<br>1001810082,pku<br>1001810083,rocky<br>1001810084,stephen<br>2002820081,sql<br>2002820082,hello<br>2002820083,hijj<br>3001810081,hhhhhhh<br>3001810082,abbbbbb </p><h1 id="文本文件-DEFLATE压缩"><a href="#文本文件-DEFLATE压缩" class="headerlink" title="文本文件,DEFLATE压缩"></a>文本文件,DEFLATE压缩</h1><p>–创建一个表，格式为文件文件：<br>CREATE TABLE student_text_def (id STRING, name STRING)<br>ROW FORMAT DELIMITED<br>    FIELDS TERMINATED BY ‘,’<br>    LINES TERMINATED BY ‘\n’<br>STORED AS TEXTFILE;<br>–设置压缩类型为Gzip压缩<br>SET hive.exec.compress.output=true;<br>SET mapred.output.compress=true;<br>SET mapred.output.compression.codec=org.apache.hadoop.io.compress.DefaultCodec;<br>–导入数据：<br>INSERT OVERWRITE TABLE student_text_def SELECT <em> FROM student;<br>–查看数据<br>SELECT </em> FROM student_text_def;<br>查看数据文件,可看到数据文件为多个.deflate文件。</p><p>hdfs dfs -ls /user/hive/warehouse/student_text_def/<br>-rw-r–r–   2015-09-16 12:48 /user/hive/warehouse/student_text_def/000000_0.deflate<br>-rw-r–r–   2015-09-16 12:48 /user/hive/warehouse/student_text_def/000001_0.deflate<br>-rw-r–r–   2015-09-16 12:48 /user/hive/warehouse/student_text_def/000002_0.deflate </p><h1 id="文本文件-Gzip压缩"><a href="#文本文件-Gzip压缩" class="headerlink" title="文本文件,Gzip压缩"></a>文本文件,Gzip压缩</h1><p>–创建一个表，格式为文件文件：<br>CREATE TABLE student_text_gzip (id STRING, name STRING)<br>ROW FORMAT DELIMITED<br>    FIELDS TERMINATED BY ‘,’<br>    LINES TERMINATED BY ‘\n’<br>STORED AS TEXTFILE;<br>–设置压缩类型为Gzip压缩<br>SET hive.exec.compress.output=true;<br>SET mapred.output.compress=true;<br>SET mapred.output.compression.codec=org.apache.hadoop.io.compress.GzipCodec;<br>–导入数据：<br>INSERT OVERWRITE TABLE student_text_gzip SELECT <em> FROM student;<br>–查看数据<br>SELECT </em> FROM student_text_gzip;<br>查看数据文件,可看到数据文件为多个.gz文件。解开.gz文件，可以看到明文文本：</p><p>hdfs dfs -ls /user/hive/warehouse/student_text_gzip/<br>-rw-r–r–  2015-09-15 10:03 /user/hive/warehouse/student_text_gzip/000000_0.gz<br>-rw-r–r–  2015-09-15 10:03 /user/hive/warehouse/student_text_gzip/000001_0.gz<br>-rw-r–r–  2015-09-15 10:03 /user/hive/warehouse/student_text_gzip/000002_0.gz<br>文本文件,Bzip2压缩</p><p>====================================<br>–创建一个表，格式为文件文件：<br>CREATE TABLE student_text_bzip2 (id STRING, name STRING)<br>ROW FORMAT DELIMITED<br>    FIELDS TERMINATED BY ‘,’<br>    LINES TERMINATED BY ‘\n’<br>STORED AS TEXTFILE;<br>–设置压缩类型为Bzip2压缩：<br>SET hive.exec.compress.output=true;<br>SET mapred.output.compress=true;<br>SET mapred.output.compression.codec=org.apache.hadoop.io.compress.BZip2Codec;<br>–导入数据<br>INSERT OVERWRITE TABLE student_text_bzip2 SELECT <em> FROM student;<br>–查看数据：<br>SELECT </em> FROM student_text_bzip2;<br>查看数据文件,可看到数据文件为多个.bz2文件。解开.bz2文件，可以看到明文文本：</p><p>hdfs dfs -ls /user/hive/warehouse/student_text_bzip2<br>-rw-r–r–  2015-09-15 10:09 /user/hive/warehouse/student_text_bzip2/000000_0.bz2<br>-rw-r–r–  2015-09-15 10:09 /user/hive/warehouse/student_text_bzip2/000001_0.bz2<br>-rw-r–r–  2015-09-15 10:09 /user/hive/warehouse/student_text_bzip2/000002_0.bz2<br>文本文件,lzo压缩</p><p>====================================<br>–创建表<br>CREATE TABLE student_text_lzo (id STRING, name STRING)<br>ROW FORMAT DELIMITED<br>    FIELDS TERMINATED BY ‘,’<br>    LINES TERMINATED BY ‘\n’<br>STORED AS TEXTFILE;<br>–设置为LZO压缩<br>SET hive.exec.compress.output=true;<br>SET mapred.output.compress=true;<br>SET mapred.output.compression.codec=com.hadoop.compression.lzo.LzopCodec;<br>–导入数据<br>INSERT OVERWRITE TABLE student_text_lzo SELECT <em> FROM student;<br>–查询数据<br>SELECT </em> FROM student_text_lzo;<br>查看数据文件,可看到数据文件为多个.lzo压缩。解开.lzo文件，可以看到明文文本。</p><p>未实测,需要安装lzop库</p><p>文本文件,lz4压缩</p><p>–创建表<br>CREATE TABLE student_text_lz4 (id STRING, name STRING)<br>ROW FORMAT DELIMITED<br>    FIELDS TERMINATED BY ‘,’<br>    LINES TERMINATED BY ‘\n’<br>STORED AS TEXTFILE;<br>–设置为LZ4压缩<br>SET hive.exec.compress.output=true;<br>SET mapred.output.compress=true;<br>SET mapred.output.compression.codec=org.apache.hadoop.io.compress.Lz4Codec;<br>–导入数据<br>INSERT OVERWRITE TABLE student_text_lz4 SELECT * FROM student;<br>查看数据文件,可看到数据文件为多个.lz4压缩。使用cat查看.lz4文件，可以看到是压缩后的文本。</p><p>hdfs dfs -ls /user/hive/warehouse/student_text_lz4<br>-rw-r–r– 2015-09-16 12:06 /user/hive/warehouse/student_text_lz4/000000_0.lz4<br>-rw-r–r– 2015-09-16 12:06 /user/hive/warehouse/student_text_lz4/000001_0.lz4<br>-rw-r–r– 2015-09-16 12:06 /user/hive/warehouse/student_text_lz4/000002_0.lz4<br>文本文件,Snappy压缩</p><p>====================================<br>–创建表<br>CREATE TABLE student_text_snappy (id STRING, name STRING)<br>ROW FORMAT DELIMITED<br>    FIELDS TERMINATED BY ‘,’<br>    LINES TERMINATED BY ‘\n’<br>STORED AS TEXTFILE;<br>–设置压缩<br>SET hive.exec.compress.output=true;<br>SET mapred.compress.map.output=true;<br>SET mapred.output.compress=true;<br>SET mapred.output.compression=org.apache.hadoop.io.compress.SnappyCodec;<br>SET mapred.output.compression.codec=org.apache.hadoop.io.compress.SnappyCodec;<br>SET io.compression.codecs=org.apache.hadoop.io.compress.SnappyCodec;<br>–导入数据<br>INSERT OVERWRITE TABLE student_text_snappy SELECT <em> FROM student;<br>–查询数据<br>SELECT </em> FROM student_text_snappy;<br>查看数据文件,可看到数据文件为多个.snappy压缩文件。使用cat查看.snappy文件，可以看到是压缩后的文本:</p><p>hdfs dfs -ls /user/hive/warehouse/student_text_snappy<br>Found 3 items<br>-rw-r–r–   2015-09-15 16:42 /user/hive/warehouse/student_text_snappy/000000_0.snappy<br>-rw-r–r–   2015-09-15 16:42 /user/hive/warehouse/student_text_snappy/000001_0.snappy<br>-rw-r–r–   2015-09-15 16:42 /user/hive/warehouse/student_text_snappy/000002_0.snappy<br>SEQUENCEFILE</p><p>Sequence文件,DEFLATE压缩</p><p>====================================<br>–创建一个表，格式为文件文件：<br>CREATE TABLE student_seq_def (id STRING, name STRING)<br>ROW FORMAT DELIMITED<br>    FIELDS TERMINATED BY ‘,’<br>    LINES TERMINATED BY ‘\n’<br>STORED AS SEQUENCEFILE;<br>–设置压缩类型为Gzip压缩<br>SET hive.exec.compress.output=true;<br>SET mapred.output.compress=true;<br>SET mapred.output.compression.codec=org.apache.hadoop.io.compress.DefaultCodec;<br>–导入数据：<br>INSERT OVERWRITE TABLE student_seq_def SELECT <em> FROM student;<br>–查看数据<br>SELECT </em> FROM student_seq_def;<br>查看数据文件,是一个密文的文件.</p><p>hdfs dfs -ls /user/hive/warehouse/student_seq_def/<br>-rw-r–r–  /user/hive/warehouse/student_seq_def/000000_0 </p><p>====================================<br>Sequence文件,Gzip压缩</p><p>–创建一个表，格式为文件文件：<br>CREATE TABLE student_seq_gzip (id STRING, name STRING)<br>ROW FORMAT DELIMITED<br>    FIELDS TERMINATED BY ‘,’<br>    LINES TERMINATED BY ‘\n’<br>STORED AS SEQUENCEFILE;<br>–设置压缩类型为Gzip压缩<br>SET hive.exec.compress.output=true;<br>SET mapred.output.compress=true;<br>SET mapred.output.compression.codec=org.apache.hadoop.io.compress.GzipCodec;<br>–导入数据：<br>INSERT OVERWRITE TABLE student_seq_gzip SELECT <em> FROM student;<br>–查看数据<br>SELECT </em> FROM student_seq_gzip;<br>查看数据文件,是一个密文的文件，无法通过gzip解压：</p><p>hdfs dfs -ls /user/hive/warehouse/student_seq_gzip/<br>-rw-r–r–  /user/hive/warehouse/student_seq_gzip/000000_0<br>RCFILE</p><p>====================================<br>RCFILE,Gzip压缩</p><p>CREATE TABLE student_rcfile_gzip (id STRING, name STRING)<br>ROW FORMAT DELIMITED<br>    FIELDS TERMINATED BY ‘,’<br>    LINES TERMINATED BY ‘\n’<br>STORED AS RCFILE; </p><p>–设置压缩类型为Gzip压缩<br>SET hive.exec.compress.output=true;<br>SET mapred.output.compress=true;<br>SET mapred.output.compression.codec=org.apache.hadoop.io.compress.GzipCodec;<br>–导入数据：<br>INSERT OVERWRITE TABLE student_rcfile_gzip SELECT id,name FROM student;<br>–查看数据<br>SELECT * FROM student_rcfile_gzip; </p><p>ORCFile</p><p>====================================<br>ORCFile有自己的参数设置压缩格式，一般不使用上述Hive参数设置压缩参数。</p><p>ORCFile,ZLIB压缩</p><p>–创建表<br>CREATE TABLE student_orcfile_zlib (id STRING, name STRING)<br>ROW FORMAT DELIMITED<br>    FIELDS TERMINATED BY ‘,’<br>    LINES TERMINATED BY ‘\n’<br>STORED AS ORCFILE TBLPROPERTIES (“orc.compress”=”ZLIB”); </p><p>–导入数据<br>INSERT OVERWRITE TABLE student_orcfile_zlib SELECT id,name FROM student;<br>–查询数据<br>SELECT * FROM student_orcfile_zlib; </p><p>ORCFILE,Snappy压缩</p><p>====================================<br>–创建表<br>CREATE TABLE student_orcfile_snappy2 (id STRING, name STRING)<br>ROW FORMAT DELIMITED<br>    FIELDS TERMINATED BY ‘,’<br>    LINES TERMINATED BY ‘\n’<br>STORED AS ORCFILE TBLPROPERTIES (“orc.compress”=”SNAPPY”); </p><p>–导入数据<br>INSERT OVERWRITE TABLE student_orcfile_snappy2 SELECT id,name FROM student;<br>–查询数据<br>SELECT * FROM student_orcfile_snappy2;<br>一般不使用下述方式。下述方式压缩后，结果与上述同类型压缩(SNAPPY)不同。具体原因待进一步研究。</p><p>–创建表<br>CREATE TABLE student_orcfile_snappy (id STRING, name STRING)<br>ROW FORMAT DELIMITED<br>    FIELDS TERMINATED BY ‘,’<br>    LINES TERMINATED BY ‘\n’<br>STORED AS ORCFILE;<br>–设置压缩<br>SET hive.exec.compress.output=true;<br>SET mapred.compress.map.output=true;<br>SET mapred.output.compress=true;<br>SET mapred.output.compression=org.apache.hadoop.io.compress.SnappyCodec;<br>SET mapred.output.compression.codec=org.apache.hadoop.io.compress.SnappyCodec;<br>SET io.compression.codecs=org.apache.hadoop.io.compress.SnappyCodec;<br>–导入数据<br>INSERT OVERWRITE TABLE student_orcfile_snappy SELECT id,name FROM student;<br>–查询数据<br>SELECT * FROM student_orcfile_snappy;<br>Parquet</p><p>====================================<br>Parquet,Snappy压缩</p><p>–创建表<br>CREATE TABLE student_parquet_snappy (id STRING, name STRING)<br>ROW FORMAT DELIMITED<br>    FIELDS TERMINATED BY ‘,’<br>    LINES TERMINATED BY ‘\n’<br>STORED AS PARQUET;<br>–设置压缩<br>SET hive.exec.compress.output=true;<br>SET mapred.compress.map.output=true;<br>SET mapred.output.compress=true;<br>SET mapred.output.compression=org.apache.hadoop.io.compress.SnappyCodec;<br>SET mapred.output.compression.codec=org.apache.hadoop.io.compress.SnappyCodec;<br>SET io.compression.codecs=org.apache.hadoop.io.compress.SnappyCodec;<br>–导入数据<br>INSERT OVERWRITE TABLE student_parquet_snappy SELECT id,name FROM student;<br>–查询数据<br>SELECT * FROM student_parquet_snappy;<br>Avro</p><p>====================================<br>Avro,Snappy压缩</p><p>–创建表<br>CREATE TABLE student_avro_snappy (id STRING, name STRING)<br>ROW FORMAT DELIMITED<br>    FIELDS TERMINATED BY ‘,’<br>    LINES TERMINATED BY ‘\n’<br>STORED AS AVRO;<br>–设置压缩<br>SET hive.exec.compress.output=true;<br>SET mapred.compress.map.output=true;<br>SET mapred.output.compress=true;<br>SET mapred.output.compression=org.apache.hadoop.io.compress.SnappyCodec;<br>SET mapred.output.compression.codec=org.apache.hadoop.io.compress.SnappyCodec;<br>SET io.compression.codecs=org.apache.hadoop.io.compress.SnappyCodec;<br>–导入数据<br>INSERT OVERWRITE TABLE student_avro_snappy SELECT id,name FROM student;<br>–查询数据<br>SELECT * FROM student_avro_snappy; </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;概述&lt;/p&gt;
&lt;p&gt;只要是配置了正确的文件类型和压缩类型(比如Textfile+Gzip、SequenceFile+Snappy等)，Hive都可以按预期读取并解析数据，提供SQL功能。&lt;/p&gt;
&lt;p&gt;SequenceFile本身的结构已经设计了内容进行压缩。所以对于Seq
      
    
    </summary>
    
    
      <category term="hive textfile seqFile RCFile ORCFile Parquet" scheme="https://www.ant-loiter.com/tags/hive-textfile-seqFile-RCFile-ORCFile-Parquet/"/>
    
  </entry>
  
  <entry>
    <title>ElasticSearch_7.X</title>
    <link href="https://www.ant-loiter.com/2019/06/04/ElasticSearch-7-X/"/>
    <id>https://www.ant-loiter.com/2019/06/04/ElasticSearch-7-X/</id>
    <published>2019-06-04T12:35:12.000Z</published>
    <updated>2020-10-30T13:49:33.118Z</updated>
    
    <content type="html"><![CDATA[<p>可以满足多用户，多场景；<br>全文检索、安全分析、聚合、地理位置； </p><p>下载： ElasticSearch + Kirblan + X-pack</p><p>es 下载后解决也可以启动：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/elasticsearch</span><br></pre></td></tr></table></figure></p><p>即可启动， 默认端口为9200</p><p>安装x-pack<br>进入到elasticsearch目录下，<br>cd ./bin/elasticsearch-plugin install file://本地目录地址； </p><p>即可； </p><p>生成安全密码命令：</p><p>./bin/x-pack/setup-passwords<br>有二个方式， 自动生成密码， 交互式生成密码； </p><p>在ElasticSearch整合kibana时， 需要配置kibana.<br>vim ./kibana/conf/。。。。<br>配置es的服务地址和登录es的用户名和密码； </p><p>启动kibana<br>./bin/kibana</p><p>get 查询类的接口<br>post 创建索引<br>put 修改索引下的文档内容<br>delete 删除索引下的文档内容<br>post _bulk 第一个是schema 第二个是body</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">精确的查找条件格式：</span><br><span class="line">_search</span><br><span class="line">&#123;</span><br><span class="line">  query:&#123;</span><br><span class="line">    match:&#123;</span><br><span class="line">      city:&quot;adf&quot;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">bool条件查询格式：</span><br><span class="line">&#123;</span><br><span class="line">  query:&#123;</span><br><span class="line">    bool:&#123;</span><br><span class="line">      must:[&#123;</span><br><span class="line">        match:&#123;</span><br><span class="line">          city:&quot;adsfadsf&quot;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;,&#123;</span><br><span class="line">        match:&#123;</span><br><span class="line">          age:23</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;]</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">不找某个条件的格式：</span><br><span class="line">_search</span><br><span class="line">&#123;query:&#123;</span><br><span class="line">  bool:&#123;</span><br><span class="line">    must_not:[]</span><br><span class="line">    &#123;</span><br><span class="line">      match:&#123;</span><br><span class="line">        city:&quot;beijing&quot;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;可以满足多用户，多场景；&lt;br&gt;全文检索、安全分析、聚合、地理位置； &lt;/p&gt;
&lt;p&gt;下载： ElasticSearch + Kirblan + X-pack&lt;/p&gt;
&lt;p&gt;es 下载后解决也可以启动：&lt;br&gt;&lt;figure class=&quot;highlight plain&quot;&gt;
      
    
    </summary>
    
    
      <category term="elasticSearch" scheme="https://www.ant-loiter.com/tags/elasticSearch/"/>
    
  </entry>
  
</feed>
